---
title: 'Activity 3: Hypothesis Testing and Power'
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(warning = FALSE,
                      echo = FALSE)

# needed for longer calculations (eg DRpower) note - in seconds
tutorial_options(exercise.timelimit = 120)
```

## Introduction

Welcome to Activity 3: **Hypothesis Testing and Power**

In this activity, we introduce the concept of statistical power and demonstrate its connection to null hypothesis testing. In many studies within malaria molecular surveillance, we aim to test specific hypotheses. For instance: Has the prevalence of drug resistance mutations increased over the past five years? Are certain genetic variants linked to gender, or occupation? Does treatment efficacy vary based on genetic markers? Each of these questions can be framed as a null hypothesis test. This leads us to a crucial question: given that a real effect exists, how likely is it that our study design will detect it? Statistical power is the mathematical way to calculate how likely we are to detect this real effect. Weâ€™ll explore these concepts using two examples: comparing drug resistance prevalence between two time points and detecting rare genetic variants


### Learning Outcomes

By the end of this tutorial, you will be able to:

- Define key terms related to null hypothesis testing.
- Use a test statistic to decide whether or not to reject a null hypothesis.
- Perform power analysis using two different statistical tests.
- Interpret power curves.
- Use minimum sample size tables.

**Disclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is artificial and designed for teaching purposes only. It does not necessarily represent the real epidemiological situation in these locations.**


## Quiz on hypothesis testing and power

```{r quiz-q1}
quiz(caption = "",
     question("What is a null hypothesis?",
           allow_retry = TRUE,
           answer("A statement that there is a significant effect or difference between groups.", correct = FALSE),
           answer("A prediction about the future outcome of an experiment.", correct = FALSE),
           answer("A statement that there is no effect or no difference between groups, and any observed effect is due to chance.", correct = TRUE),
           answer("A hypothesis that describes the expected relationship between two variables.", correct = FALSE), 
           correct = "That is correct! A null hypothesis assumes there is no effect/difference between groups. Often the null hypothesis is a statement that there is nothing interesting going on - but not always! For example, imagine our null hypothesis is that people using bed nets have the same malaria incidence as those not using bed nets. If this null hypothesis is true then that is very interesting indeed.", 
           incorrect = "Hint: when you see the word 'null' think 'nothing'."
  )
)
```

```{r quiz-q2}
quiz(caption = "",
     question("Which of these is *not* a null hypothesis?",
           allow_retry = TRUE,
           answer("There is no difference in malaria prevalence between people who sleep under bed nets and people who do not.", correct = FALSE),
           answer("The presence of a genetic marker for drug resistance is independent of the region (e.g., East Africa vs. West Africa).", correct = FALSE),
           answer("Elevation has no linear relationship with malaria risk.", correct = FALSE),
           answer("Malaria incidence is twice as high in men as it is in women.", correct = TRUE), 
           correct = "That is correct! This cannot be a null hypothesis as it expresses a difference between two groups. The null hypothesis would be that there is no difference in incidence between men and women.", 
           incorrect = "Hint: for each statement, try to work out if it assumes an effect/difference or no effect between groups."
  )
)
```

```{r quiz-q3}
quiz(caption = "",
     question("A false-negative result is when...",
           allow_retry = TRUE,
           answer("You fail to reject the null hypothesis when it is actually false.", correct = TRUE),
           answer("You fail to reject the null hypothesis when it is actually true.", correct = FALSE, message = "You're right that this is a negative result, but this is a true-negative rather than a false-negative."),
           answer("You reject the null hypothesis when it is actually false.", correct = FALSE, message = "This is the opposite of a false-negative result; this is a true-positive result."),
           answer("You reject the null hypothesis when it is actually true.", correct = FALSE, message = "You're right that this is a false result, but it is a false-positive rather than a false-negative."), 
           correct = "That is correct! A false-negative result means we failed to detect an effect that was really there."
  )
)
```

```{r quiz-q4}
quiz(caption = "",
     question("The parameter $\\alpha$ is often referred to as...",
           allow_retry = TRUE,
           answer("The confidence level of a hypothesis test.", correct = FALSE),
           answer("The significance level of a hypothesis test.", correct = TRUE),
           answer("The power of a statistical test.", correct = FALSE),
           answer("The probability of making a Type II error.", correct = FALSE), 
           correct = "That is correct! Smaller values of $\\alpha$ make it harder for us to reject the null hypothesis.", 
           incorrect = "Hint: $\\alpha$ is the threshold we set to decide if a result is statistically significant."
  )
)
```

```{r quiz-q5}
quiz(caption = "",
     question("A statistical test that only examines effects in one direction is called...",
           allow_retry = TRUE,
           answer("A one-headed test.", correct = FALSE),
           answer("A one-way street analysis.", correct = FALSE),
           answer("A wild goose chase.", correct = FALSE),
           answer("A one-tailed test.", correct = TRUE), 
           correct = "That is correct! A one-tailed test assumes that we already know the direction of the effect - for example, that bed nets will decrease the incidence of malaria, not increase it. One-tailed tests tend to be easier to disprove, but we have to be sure about the direction of the effect.", 
           incorrect = "Hint: the 'tails' of a distribution refer to the low probability regions on either side of the main hump."
  )
)
```

```{r quiz-q6}
quiz(caption = "",
     question_radio("In statistical testing, we always compare our test statistic against the normal distribution.",
           allow_retry = TRUE,
           answer("TRUE", correct = FALSE),
           answer("FALSE", correct = TRUE),
           correct = "That is correct! Different statistical tests can have different distributions for the test statistic - for example, the chi-squared test statistic follows a chi-squared distribution. We need to know the correct distribution to compare against when assessing significance.", 
           incorrect = "Hint: is a test statistic always normally distributed?"
  )
)
```

```{r quiz-q7}
quiz(caption = "",
     question("You are running a study to test if the prevalence of a drug resistant mutation has changed over time. You analyse your data using a z-test. The critical values for a two-tailed z-test at the significance level $\\alpha = 0.05$ are at $\\pm1.96$. You obtain a test statistic of -2.54. What should you do based on this result?",
           allow_retry = TRUE,
           answer("Reject the null hypothesis because the test statistic is less than zero.", correct = FALSE),
           answer("Reject the null hypothesis because the test statistic is less than the lower critical value.", correct = TRUE),
           answer("Fail to reject the null hypothesis because the test statistic is negative.", correct = FALSE),
           answer("Fail to reject the null hypothesis because the test statistic is less than the upper critical value.", correct = FALSE), 
           correct = "That is correct! The test statistic is negative, meaning we are comparing against the lower critical value of -1.96. The test statistic is greater in magnitude than the critical value, so we should reject the null hypothesis.", 
           incorrect = "Hint: we reject the null hypothesis when the observed value is outside the range -1.96 to +1.96."
  )
)
```

Well done on completing this quiz! We will now put some of these ideas into practice.

## Testing for changes in drug resistance prevalence over time

You are a local health minister working in the Gombe region of Nigeria. You are concerned that the prevalence of antimalarial resistance may be increasing in Gombe city, the capital of Gombe state. A study conducted three years ago found that the prevalence of *pfmdr1* N86Y mutations was 15%. You plan to conduct a new study to establish if there has been a significant change in the prevalence of N86Y mutations over this time.

We will take the previous estimate of 15% prevalence three years ago to be exact. Because we are comparing a sample against a known value, the appropriate statistical test is the one-sample z-test for proportions.

```{r ztest-quiz}
quiz(caption = "",
     question("Which of these is the correct null hypothesis under the planned test?",
           allow_retry = TRUE,
           answer("The prevalence of *pfmdr1* N86Y mutations has increased from 15% over the past three years.", correct = FALSE),
           answer("The prevalence of *pfmdr1* N86Y mutations has decreased from 15% over the past three years.", correct = FALSE),
           answer("The prevalence of *pfmdr1* N86Y mutations is still 15%, the same as it was three years ago.", correct = TRUE),
           answer("The prevalence of *pfmdr1* N86Y mutations has changed from its earlier value of 15%.", correct = FALSE), 
           correct = "That is correct! It is the only option that specifies no effect/difference in the mutation prevalence between the two time periods.", 
           incorrect = "Hint: remember that a null hypothesis is a statement of no effect or difference between groups."
  )
)
```

We will denote the prevalence three years ago by $p_0 = 0.15$. For our power analysis, we need to assume a known prevalence in the present day, which we will denote $p_1$. We will be pessimistic and assume that the prevalence has doubled over the three year period, meaning $p_1=0.30$. Our initial plan is to use a sample size of $N=150$.

Given these values, we can calculate the *expected value* for our test statistic using the following formula:
$$
\mu = \frac{|p_1 - p_0|}{\sqrt{\frac{p_1(1-p_1)}{N}}}
$$
The vertical lines around $|p_1 - p_0|$ mean we should take the *absolute value* of the difference. This ensures that $\mu$ is always positive.

Complete the following R code to calculate the value of $\mu$:

```{r ztest-1, exercise=TRUE}
# input parameters
p0 <- 
p1 <- 
N <- 

# calculate absolute value of difference in prevalence
p_diff <- abs(p1 - p0)

# calculate the standard error
SE <- sqrt(p1*(1 - p1) / N)

# calculate mu
mu <- 

print(mu)
```

```{r ztest-1-solution}
# input parameters
p0 <- 0.15
p1 <- 0.3
N <- 150

# calculate absolute value of difference in prevalence
p_diff <- abs(p1 - p0)

# calculate the standard error
SE <- sqrt(p1*(1 - p1) / N)

# calculate mu
mu <- p_diff / SE

print(mu)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $\mu = 4.00$.

</details>
</br>

This means that *on average* we should expect our test statistic to equal $4.00$ if the alternative hypothesis is true. The actual value that we observe would be expected to vary around this value.

We can use $\mu$ to tell us our power. The formula for power under the z-test is:

$$
\text{Power} = 1 - \phi(z_{1-\alpha/2} - \mu)
$$

In this formula, $\phi(x)$ refers to the area under the curve of a standard normal distribution up to the point $x$. There is no simple way of calculating this value, but we can obtain it easily in R using the `pnorm()` function. As in previous activities, the value $z_{1 - \alpha/2}$ refers to the critical value of the normal distribution at a significance level $\alpha$ (two-tailed), which is approximately equal to 1.96.

Complete the following R code to calculate the power under the planned study design:

```{r ztest-2-params}
p0 <- 0.15
p1 <- 0.3
N <- 150
mu <- abs(p1 - p0) / sqrt(p1*(1 - p1) / N)
```

```{r ztest-2, exercise=TRUE, exercise.setup = "ztest-2-params"}
# calculate power using the known value of mu
z <- 1.96
power <- # hint, you will need to use the pnorm() function here

print(power)
```

```{r ztest-2-solution}
# calculate power using the known value of mu
z <- 1.96
power <- 1 - pnorm(z - mu)

print(power)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $\text{Power} = 0.98$.

</details>
</br>

```{r power-quiz}
quiz(caption = "",
     question("A power of 98% means...",
           allow_retry = TRUE,
           answer("There is a 98% chance that the study will be successful.", correct = FALSE),
           answer("There is a 98% difference in prevalence between the time points.", correct = FALSE),
           answer("There is a 98% chance that the alternative hypothesis is true.", correct = FALSE),
           answer("Assuming the alternative hypothesis is true, there is a 98% chance that we will correctly reject the null hypothesis.", correct = TRUE), 
           correct = "That is correct! Under the current study design there is a 98% chance that we will correctly conclude that prevalence has changed over the three year period, assuming that it doubled from 15% to 30% over this time.", 
           incorrect = "Hint: power is the chance of correctly rejecting the null hypothesis."
  )
)
```


## Using power curves

We have calculated that our current study design has 98% power. We normally aim for at least 80% power, meaning this study is adequately powered. In fact, we could argue that it is *over-powered*, meaning we could get away with using fewer samples and still have a good chance of detecting a real effect.

We can use power curves to explore how power changes as a function of sample size. In the plot below, the region with at least 80% power is shaded in blue.

```{r, fig.width=6, fig.height=4}
data.frame(p0 = 0.15,
           p1 = 0.3,
           N = 10:150,
           z = 1.96) |>
  mutate(mu_alt = abs(p1 - p0) / sqrt(p1*(1 - p1) / N),
           pow = 1 - pnorm(z - mu_alt)) |>
  ggplot() + theme_bw() +
  geom_ribbon(aes(x = N, ymin = 0.8, ymax = 1), fill = "dodgerblue", alpha = 0.2) +
  geom_line(aes(x = N, y = pow)) +
  scale_x_continuous(limits = c(10, 150), breaks = seq(0, 150, 10), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2), expand = c(0, 0)) +
  xlab("Sample size (N)") + ylab("Power")
```

```{r power-curve-quiz}
quiz(caption = "",
     question("From this graph, what is the minimum sample size needed to achieve 80% power?",
           allow_retry = TRUE,
           answer("60", correct = FALSE),
           answer("74", correct = TRUE),
           answer("100", correct = FALSE),
           answer("More than 150", correct = FALSE), 
           correct = "That is correct! Our planned sample size of 150 is more than double what we need to achieve 80% power.", 
           incorrect = "Hint: look for the point at which the curve intersects the shaded region."
  )
)
```

The power analysis indicates that we may not need to sequence 150 samples after all. However, it did make the rather pessimistic assumption that prevalence has doubled from 15% to 30% over the three year period. It would be useful to repeat this analysis making different assumptions about the prevalence. One way to do this is via a series of power curves:

```{r, fig.width=6, fig.height=4}
expand_grid(p0 = 0.15,
           p1 = seq(0.2, 0.5, 0.05),
           N = 10:150,
           z = 1.96) |>
  mutate(mu_alt = abs(p1 - p0) / sqrt(p1*(1 - p1) / N),
         pow = 1 - pnorm(z - mu_alt),
         p1_percent = sprintf("%s%%", p1*100)) |>
  ggplot() + theme_bw() +
  geom_ribbon(aes(x = N, ymin = 0.8, ymax = 1), fill = "dodgerblue", alpha = 0.2) +
  geom_line(aes(x = N, y = pow, col = p1_percent, group = p1_percent)) +
  scale_x_continuous(limits = c(10, 150), breaks = seq(0, 150, 10), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2), expand = c(0, 0)) +
  labs(color = "Present day\nprevalence") +
  xlab("Sample size (N)") + ylab("Power")
```

Even a slight change in our assumption about the prevalence of N86Y mutations has a large effect on power. If we assumed present-day prevalence was 25% rather than 30% then we would need twice as many samples to achieve 80% power. This analysis was useful for exploring the exact relationship between sample size and power. However, it can be fiddly to read values off the power curve to find the exact point at which it crosses the 80% threshold. This is where sample size formulae and sample size tables come in handy. 

## Using sample size tables

Recall that power under the one-sample z-test for proportions was given by
$$
\text{Power} = 1 - \phi\left(z_{1-\alpha/2} - \frac{|p_1 - p_0|}{\sqrt{\frac{p_1(1-p_1)}{N}}} \right)
$$
Recall that in an earlier module we rearranged the formula for the Wald confidence interval to arrive at a new formula in terms of the sample size $N$. Here, we want to do exactly the same thing, just with a more complicated formula! We won't walk through the steps of this derivation, but take my word for it that this can be rearranged to produce:
$$
N = (z_{1 - \alpha/2} + z_{1 - \beta})^2\frac{p_1(1 - p_1)}{(p_1 - p_0)^2}
$$
The only unfamiliar term here is $z_{1 - \beta}$, which is the area under the curve of the standard normal distribution up to the value $1 - \beta$. The parameter $\beta$ is one minus our power (typically $\beta = 0.2$ because minimum power is usually 0.8).

The following R code implements this sample size formula. Have a play around with this code. Try changing the value of `p1` and see what happens to the minimum sample size. Do you obtain the value $N=74$ when $p_2=0.3$, like we found from the power curve? Remember that you should always round sample sizes *up* if they are non-integers. What happens to the sample size as `p1` gets closer to 15%?

```{r ztest-ss-2, exercise=TRUE}
# define our assumed values
p0 <- 0.15
p1 <- 0.30

# define the two z parameters
z_alpha <- 1.96
z_beta <- qnorm(0.8)

# calculate the minimum sample size
(z_alpha + z_beta)^2 * p1*(1 - p1) / (p1 - p0)^2
```

One of the nice things about sample size formulae is that we can use them to produce tables of minimum sample sizes. **Table 1** shows the minimum sample size required to achieve 80% power under different assumptions about the prevalence of N86Y mutations:

```{r}
data.frame(p0 = 0.15,
           p1 = seq(0.2, 0.5, 0.05),
           z_alpha = 1.96,
           z_beta = qnorm(0.8)) |>
  mutate(N = ceiling((z_alpha + z_beta)^2 * p1*(1 - p1) / (p1 - p0)^2),
         p1_percent = sprintf("%s%%", p1*100)) |>
  select(p1_percent, N) |>
  kable("html", col.names = c("Assumed present-day prevalence of N86Y mutation", "Sample size"), escape = FALSE) |>
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) |>
  column_spec(1, width = "6cm") |>
  column_spec(2, width = "4cm") |>
  footnote(general = "\nTable 1: minimum sample sizes under various assumptions about present-day prevalence", general_title = "")
```

This allows us to scan across values and work out what is reasonable, and also what is achievable.

<div style="padding: 10px; border-radius: 5px; background-color: #fef3e7;">
  <span style="font-size: 1.2em; color: #d19554;">
    <i class="fas fa-comment"></i> Reflection:
  </span> 

  <span style="color:#d19554;"> 
  What sample size would you opt for in this case? What factors could help you decide on a reasonable assumption for the present day prevalence of N86Y mutations? What factors could help constrain which sample sizes are feasible?
  </span>
</div>

## Detecting rare variants

Building on the success of your N86Y study in Gombe city, you have been invited to conduct a new investigation focused on identifying *pfk13* mutations in the nearby town of Pindiga. This study will target WHO validated mutations that are known to be associated with partial resistance to artemisinin. Instead of estimating the *prevalence* of these mutations, your goal is simply to determine whether any of these mutations are *present* in the population.

Your will test individuals who present with malaria symptoms at a local health facility. For those who test positive for malaria, dried blood spots will be collected and subsequently sent for sequencing. However, due to limited resources, you are only able to sequence 100 samples.

The key question, then, is: **with a sample size of only 100, is it worthwhile to pursue this study?**

### Framing the problem as a hypothesis test

This type of detection study can be framed as a hypothesis test. The null hypothesis is that *there are no WHO-validated mutations present in the population*. In other words, the prevalence of these mutations is zero. Even a single observation of a mutant would disprove this null hypothesis. Therefore, unlike most statistical tests, there is no test statistic to calculate. Instead, we simply reject the null hypothesis if we see a single WHO-validated mutant.

It is straightforward to calculate power under this test. We start by assuming a known prevalence, $p$, for the validated mutations. $p$ is zero under the null hypothesis, and $p$ must be greater than 0 under the alternative hypothesis. We will use $N$ to denote the sample size. We can derive the power by following these steps:

**Step 1: Chance of a single negative result:**

The probability that a single sample is negative (i.e. does not carry a validated *pfk13* mutation) is given by:
$$
\text{Pr}(\text{Negative}) = 1 - p
$$
**Step 2: Chance of two negative results:**

The probability of two negative samples is the probability of one negative sample multiplied by the probability of another negative sample:
$$
\begin{align}
\text{Pr}(\text{Two negatives}) &= (1 - p)(1 - p) \\
&= (1 - p)^2
\end{align}
$$

**Step 3: Chance of $N$ negative results:**

The probability that all $N$ samples are negative is the probability of one negative sample raised to the power $N$:
$$
\text{Pr}(N \text{ negatives}) = (1 - p)^N
$$
This is like saying what is the probability we observe a negative outcome $N$ times. It assumes that samples are drawn independently from a much larger population.

**Step 4: Chance of at least one positive result:**

The chance of seeing at least one positive sample is equal to one minus the probability of seeing no positive samples. If we see a positive sample then we reject the null hypothesis. Hence, this is also our power:

$$
\text{Power} = 1 - (1 - p)^N
$$
This very simple expression can be used to guide our study design.

Complete the following R code to implement this formula. What is our power if we assume a prevalence of 5%?

```{r detection-1, exercise=TRUE}
# define parameters
p <- 
N <- 

# calculate power
power <- 

print(power)
```

```{r detection-1-solution}
# define parameters
p <- 0.05
N <- 100

# calculate power
power <- 1 - (1 - p)^N

print(power)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $\text{Power} = 0.994$ if we assume a prevalence of $p = 0.05$.

</details>
</br>

Based on this result, we are over-powered to detect *pfk13* mutants, meaning we can reduce the sample size. So, how many samples are needed? Rearranging our power formula to be in terms of $N$ we obtain:
$$
N = \frac{\text{log}(1 - \text{Power})}{\text{log}(1 - p)}
$$

Complete the following R code to implement this sample size formula. What minimum sample size is needed if we assume a prevalence of 5% and we are aiming for 80% power?

```{r detection-2, exercise=TRUE}
# define prevalence
p <- 

# calculate minimum sample size
N <- 

print(N)
```

```{r detection-2-solution}
# define prevalence
p <- 0.05

# calculate minimum sample size
N <- log(1 - 0.8) / log(1 - p)

print(N)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $N = 31.38$, which would be rounded up to $N = 32$.

</details>
</br>

This is a very promising result - we can definitely run a well-powered study within our resource constraints. However, we did make the fairly pessimistic assumption that validated *pfk13* mutants are already at 5% prevalence in the population. In reality, we may want to catch them before they reach 5% in order to take pre-emptive measures. Again, we can consult a sample size table (**Table 2**):

```{r}
data.frame(p = c(0.001, 0.005, seq(0.01, 0.05, 0.01))) |>
  mutate(N = ceiling(log(1 - 0.8) / log(1 - p)),
         p_percent = sprintf("%s%%", p*100)) |>
  select(p_percent, N) |>
  kable("html", col.names = c("Assumed prevalence of pfk13 mutations", "Sample size"), escape = FALSE) |>
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) |>
  column_spec(1, width = "5cm") |>
  column_spec(2, width = "4cm") |>
  footnote(general = "\nTable 2: minimum sample sizes under various assumptions about pfk13 mutation prevalence", general_title = "")
```

<div style="padding: 10px; border-radius: 5px; background-color: #fef3e7;">
  <span style="font-size: 1.2em; color: #d19554;">
    <i class="fas fa-comment"></i> Reflection:
  </span> 

  <span style="color:#d19554;"> 
  Based on the values in **Table 2**, do you think it is worthwhile to conduct this study? If you ran a study that was powered down to 2% prevalence (80 samples) and did not find any *pfk13* mutants, would you be reassured by this result?
  </span>
</div>
