---
title: 'Activity 3: Hypothesis Testing and Power'
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(warning = FALSE,
                      echo = FALSE)

# needed for longer calculations (eg DRpower) note - in seconds
tutorial_options(exercise.timelimit = 120)
```

## Introduction

Welcome to Activity 3: **Hypothesis Testing and Power**

In this activity, we introduce the concept of statistical power and demonstrate its connection to null hypothesis testing. In many studies within malaria molecular surveillance, we aim to test specific hypotheses. For instance: Has the prevalence of drug resistance mutations increased over the past five years? Are certain genetic variants linked to gender, or occupation? Does treatment efficacy vary based on genetic markers? Each of these questions can be framed as a null hypothesis test. This leads us to a crucial question: given that a real effect exists, how likely are we to detect it? Statistical power is the mathematical way to evaluate how likely we are to detect this real effect. Weâ€™ll explore these concepts using two examples: comparing drug resistance prevalence against a known value and detecting rare genetic variants


### Learning Outcomes

By the end of this tutorial, you will be able to:

- Define key terms related to null hypothesis testing.
- Use a test statistic to decide whether or not to reject a null hypothesis.
- Perform power analysis using two different statistical tests.
- Interpret power curves.
- Use minimum sample size tables.

*Disclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is artificial and designed for teaching purposes only. It does not necessarily represent the real epidemiological situation in these locations.*


## Quiz on hypothesis testing and power

```{r quiz-q1}
quiz(caption = "QUIZ - Hypothesis testing and power",
     question("What is a null hypothesis?",
           allow_retry = TRUE,
           answer("A statement that there is a significant effect or difference between groups.", correct = FALSE),
           answer("A prediction about the future outcome of an experiment.", correct = FALSE),
           answer("A statement that there is no effect or no difference between groups, and any observed effect is due to chance.", correct = TRUE),
           answer("A hypothesis that describes the expected relationship between two variables.", correct = FALSE), 
           correct = "That is correct! A null hypothesis assumes there is no effect/difference between groups. Often the null hypothesis is a statement that there is nothing interesting going on - but not always! For example, imagine our null hypothesis is that people using bed nets have the same malaria incidence as those not using bed nets. If this null hypothesis is true then that is very interesting indeed.", 
           incorrect = "Hint: when you see the word 'null' think 'nothing'."
  ),
  question("Which of these is *not* a null hypothesis?",
           allow_retry = TRUE,
           answer("There is no difference in malaria prevalence between people who sleep under bed nets and people who do not.", correct = FALSE),
           answer("The presence of a genetic marker for drug resistance is independent of the region (e.g., East Africa vs. West Africa).", correct = FALSE),
           answer("Elevation has no linear relationship with malaria risk.", correct = FALSE),
           answer("Malaria incidence is twice as high in men as it is in women.", correct = TRUE), 
           correct = "That is correct! This cannot be a null hypothesis as it expresses a difference between two groups. The null hypothesis would be that there is no difference in incidence between men and women.", 
           incorrect = "Hint: for each statement, try to work out if it assumes an effect/difference or no effect between groups."
  ),
  question("A false-negative result is when...",
           allow_retry = TRUE,
           answer("You fail to reject the null hypothesis when it is actually false.", correct = TRUE),
           answer("You fail to reject the null hypothesis when it is actually true.", correct = FALSE, message = "You're right that this is a negative result, but this is a true-negative rather than a false-negative."),
           answer("You reject the null hypothesis when it is actually false.", correct = FALSE, message = "This is the opposite of a false-negative result; this is a true-positive result."),
           answer("You reject the null hypothesis when it is actually true.", correct = FALSE, message = "You're right that this is a false result, but it is a false-positive rather than a false-negative."), 
           correct = "That is correct! A false-negative result means we failed to detect an effect that was really there."
  ),
  question("The parameter $\\alpha$ is often referred to as...",
           allow_retry = TRUE,
           answer("The confidence level of a hypothesis test.", correct = FALSE),
           answer("The significance level of a hypothesis test.", correct = TRUE),
           answer("The power of a statistical test.", correct = FALSE),
           answer("The probability of making a Type II error.", correct = FALSE), 
           correct = "That is correct! Smaller values of $\\alpha$ make it harder for us to reject the null hypothesis.", 
           incorrect = "Hint: $\\alpha$ is the threshold we set to decide if a result is statistically significant."
  ),
  question("A statistical test that only examines effects in one direction is called...",
           allow_retry = TRUE,
           answer("A one-headed test.", correct = FALSE),
           answer("A one-way street analysis.", correct = FALSE),
           answer("A wild goose chase.", correct = FALSE),
           answer("A one-tailed test.", correct = TRUE), 
           correct = "That is correct! A one-tailed test assumes that we already know the direction of the effect - for example, that bed nets will decrease the incidence of malaria, not increase it. One-tailed tests tend to be easier to disprove, but we have to be sure about the direction of the effect.", 
           incorrect = "Hint: the 'tails' of a distribution refer to the low probability regions on either side of the main hump."
  ),
  question_radio("In statistical testing, we always compare our test statistic against the normal distribution.",
           allow_retry = TRUE,
           answer("TRUE", correct = FALSE),
           answer("FALSE", correct = TRUE),
           correct = "That is correct! Different statistical tests can have different distributions for the test statistic - for example, the chi-squared test statistic follows a chi-squared distribution. We need to know the correct distribution to compare against when assessing significance.", 
           incorrect = "Hint: is a test statistic always normally distributed?"
  ),
  question("You are running a study to test if the prevalence of a drug resistant mutation has changed over time. You analyse your data using a z-test. The critical values for a two-tailed z-test at the significance level $\\alpha = 0.05$ are at $\\pm1.96$. You obtain a test statistic of -2.54. What should you do based on this result?",
           allow_retry = TRUE,
           answer("Reject the null hypothesis because the test statistic is less than zero.", correct = FALSE),
           answer("Reject the null hypothesis because the test statistic is less than the lower critical value.", correct = TRUE),
           answer("Fail to reject the null hypothesis because the test statistic is negative.", correct = FALSE),
           answer("Fail to reject the null hypothesis because the test statistic is less than the upper critical value.", correct = FALSE), 
           correct = "That is correct! The test statistic is negative, meaning we are comparing against the lower critical value of -1.96. The test statistic is greater in magnitude than the critical value, so we should reject the null hypothesis.", 
           incorrect = "Hint: we reject the null hypothesis when the observed value is outside the range -1.96 to +1.96."
  )
)
```

Well done on completing this quiz! We will now put some of these ideas into practice.

## Testing for changes in molecular markers of drug resistance over time

You are a local health minister working in the Gombe region of Nigeria. You are concerned that the prevalence of antimalarial resistance may be increasing in Gombe city, the capital of Gombe state. A study conducted three years ago found that the prevalence of *pfmdr1* N86Y mutations was 15%. You plan to conduct a new study to determine if there has been a significant change in the prevalence of N86Y mutations over this three year period.

To simplify our analysis, we will assume the estimate of 15% prevalence three years ago is exactly correct. This means we are comparing a sample in the present day against a single known value. The appropriate statistical test here is the **one-sample z-test for proportions**.

```{r ztest-quiz}
quiz(caption = "QUIZ - Null hypothesis under the z-test",
     question("Which of these is the correct null hypothesis under the planned test?",
           allow_retry = TRUE,
           answer("The prevalence of *pfmdr1* N86Y mutations has increased from 15% over the past three years.", correct = FALSE),
           answer("The prevalence of *pfmdr1* N86Y mutations has decreased from 15% over the past three years.", correct = FALSE),
           answer("The prevalence of *pfmdr1* N86Y mutations is still 15%, the same as it was three years ago.", correct = TRUE),
           answer("The prevalence of *pfmdr1* N86Y mutations has changed from its earlier value of 15%.", correct = FALSE), 
           correct = "That is correct! It is the only option that specifies no effect/difference in the mutation prevalence between the two time periods.", 
           incorrect = "Hint: remember that a null hypothesis is a statement of no effect or difference between groups."
  )
)
```

The test statistic under the z-test is given by:

$$
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}}
$$

where $p_0$ is the value we are comparing against (in our case $p_0 = 0.15$). If the null hypothesis is true, and the prevalence is indeed $p_0$, then $Z$ should follow a standard normal distribution. This means there is only a 5% chance that we will see values greater than $+1.96$, or less than $-1.96$.

But what if the null hypothesis is false? We can explore this question by specifying an *alternative* hypothesis. To do this, we need to specify a known prevalence in the present day, which we will call $p$. We will be pessimistic and assume that the prevalence has doubled over the three year period from $p_0=0.15$ to $p=0.30$.

Given these values, we can calculate the *expected value* for our test statistic under the alternative hypothesis:
$$
E[Z] = \frac{|p - p_0|}{\sqrt{\frac{p(1-p)}{n}}}
$$
The vertical lines around $|p - p_0|$ indicate that we should take the *absolute value* of the difference. This ensures that $E[Z]$ is always positive.

Complete the following R code to calculate the value of $E[Z]$, assuming a sample size of $n = 150$.

```{r ztest-1, exercise=TRUE}
# input parameters
p0 <- 
p <- 
n <- 

# calculate absolute value of difference in prevalence
p_diff <- abs(p - p0)

# calculate the standard error
SE <- sqrt(p*(1 - p) / n)

# calculate the expected value of the statistic
E_Z <- 

print(E_Z)
```

```{r ztest-1-solution}
# input parameters
p0 <- 0.15
p <- 0.3
n <- 150

# calculate absolute value of difference in prevalence
p_diff <- abs(p - p0)

# calculate the standard error
SE <- sqrt(p*(1 - p) / n)

# calculate the expected value of the statistic
E_Z <- p_diff / SE

print(E_Z)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $E[Z}] = 4.01$.

</details>
</br>

This means that *on average* we should expect our test statistic to equal $4.01$ if the alternative hypothesis is true.

We can use $E[Z]$ to tell us our power. The formula for power under the z-test is:

$$
\text{Power} = 1 - \phi(z_{1-\alpha/2} - E[Z])
$$

$\phi()$ in this formula refers to the area under the curve of a standard normal distribution. There is no simple way of calculating this value, but we can obtain it easily in R using the `pnorm()` function. As in previous activities, the value $z_{1 - \alpha/2}$ refers to the critical value of the normal distribution at a significance level $\alpha$ (two-tailed), which is approximately equal to 1.96.

Complete the following R code to calculate the power under the planned study design. You still have access to the variable `E_Z` from before in this code chunk:

```{r ztest-2-params}
p0 <- 0.15
p <- 0.3
n <- 150
E_Z <- abs(p - p0) / sqrt(p*(1 - p) / n)
```

```{r ztest-2, exercise=TRUE, exercise.setup = "ztest-2-params"}
# calculate power using the known value of E_Z
z_alpha <- 1.96
power <- # hint, you will need to use the pnorm() function here

print(power)
```

```{r ztest-2-solution}
# calculate power using the known value of E_Z
z_alpha <- 1.96
power <- 1 - pnorm(z_alpha - E_Z)

print(power)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $\text{Power} = 0.98$.

</details>
</br>

```{r power-quiz}
quiz(caption = "QUIZ - Interpreting power",
     question("A power of 98% means...",
           allow_retry = TRUE,
           answer("There is a 98% chance that the study will be successful.", correct = FALSE),
           answer("There is a 98% difference in prevalence between the time points.", correct = FALSE),
           answer("There is a 98% chance that the alternative hypothesis is true.", correct = FALSE),
           answer("Assuming the alternative hypothesis is true, there is a 98% chance that we will correctly reject the null hypothesis.", correct = TRUE), 
           correct = "That is correct! Under the current study design there is a 98% chance that we will correctly conclude that prevalence has changed over the three year period, assuming that it doubled from 15% to 30% over this time.", 
           incorrect = "Hint: power is the chance of correctly rejecting the null hypothesis."
  )
)
```


## Using power curves

Our current study design has 98% power. We normally aim for at least 80% power, meaning this study is adequately powered. In fact, we could argue that it is *over-powered*, meaning we could get away with using fewer samples and still have a good chance of detecting a real effect.

We can use power curves to explore how power changes as a function of sample size. In the plot below, the region with at least 80% power is shaded in blue.

```{r, fig.width=6, fig.height=4}
data.frame(p0 = 0.15,
           p1 = 0.3,
           N = 10:150,
           z = 1.96) |>
  mutate(mu_alt = abs(p1 - p0) / sqrt(p1*(1 - p1) / N),
           pow = 1 - pnorm(z - mu_alt)) |>
  ggplot() + theme_bw() +
  geom_ribbon(aes(x = N, ymin = 0.8, ymax = 1), fill = "dodgerblue", alpha = 0.2) +
  geom_line(aes(x = N, y = pow)) +
  scale_x_continuous(limits = c(10, 150), breaks = seq(0, 150, 10), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2), expand = c(0, 0)) +
  xlab("Sample size (n)") + ylab("Power")
```

```{r power-curve-quiz}
quiz(caption = "QUIZ - Reading off power curves",
     question("From this graph, what is the minimum sample size needed to achieve 80% power?",
           allow_retry = TRUE,
           answer("60", correct = FALSE),
           answer("74", correct = TRUE),
           answer("100", correct = FALSE),
           answer("More than 150", correct = FALSE), 
           correct = "That is correct! Our planned sample size of 150 is more than double what we need to achieve 80% power.", 
           incorrect = "Hint: look for the point at which the curve intersects the shaded region."
  )
)
```

The power analysis indicates that we may not need to sequence 150 samples after all. However, it did make the rather pessimistic assumption that prevalence has doubled from 15% to 30% over the three year period. It would be useful to repeat this analysis, exploring different assumptions about the prevalence. One way to do this is via a series of power curves:

```{r, fig.width=6, fig.height=4}
expand_grid(p0 = 0.15,
           p1 = seq(0.2, 0.5, 0.05),
           N = 10:150,
           z = 1.96) |>
  mutate(mu_alt = abs(p1 - p0) / sqrt(p1*(1 - p1) / N),
         pow = 1 - pnorm(z - mu_alt),
         p1_percent = sprintf("%s%%", p1*100)) |>
  ggplot() + theme_bw() +
  geom_ribbon(aes(x = N, ymin = 0.8, ymax = 1), fill = "dodgerblue", alpha = 0.2) +
  geom_line(aes(x = N, y = pow, col = p1_percent, group = p1_percent)) +
  scale_x_continuous(limits = c(10, 150), breaks = seq(0, 150, 10), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2), expand = c(0, 0)) +
  labs(color = "Present day\nprevalence (p)") +
  xlab("Sample size (n)") + ylab("Power")
```

Even a slight change in our assumption about the prevalence of N86Y mutations has a large effect on power. If we assumed present-day prevalence was 25% rather than 30% then we would need close to 150 samples to achieve 80% power. This analysis was useful for exploring the exact relationship between sample size and power. However, it can be fiddly to read values off the power curve to find the exact point at which it crosses the 80% threshold. This is where sample size formulae and sample size tables come in handy. 

## Using sample size tables

Recall that power under the one-sample z-test for proportions is given by
$$
\text{Power} = 1 - \phi\left(z_{1-\alpha/2} - \frac{|p - p_0|}{\sqrt{\frac{p(1 - p)}{n}}} \right)
$$
In an earlier module we rearranged the formula for the Wald confidence interval to arrive at a new formula in terms of the sample size $n$. Here, we want to do exactly the same thing, just with a more complicated formula! We won't walk through the steps of rearranging this formula, but take my word for it that this can be rearranged to produce:
$$
n = (z_{1 - \alpha/2} + z_{1 - \beta})^2\frac{p(1 - p)}{(p - p_0)^2}
$$
The only unfamiliar term here is $z_{1 - \beta}$, which is the area under the curve of the standard normal distribution up to the value $1 - \beta$. The parameter $\beta$ is defined as one minus our power. Typically $\beta = 0.2$ because target power is usually 80%.

The following R code implements this sample size formula. Have a play around with this code. Try changing the value of `p` and see what happens to the minimum sample size. Do you obtain the value $n=74$ when $p=0.3$, like we found from the power curve? Remember that you should always round sample sizes *up* if they are non-integers. What happens to the sample size as `p` gets closer to the 15% threshold?

```{r ztest-ss-2, exercise=TRUE}
# define our assumed values
p0 <- 0.15
p <- 0.30

# define the two z parameters
z_alpha <- 1.96
z_beta <- qnorm(0.8)

# calculate the minimum sample size
(z_alpha + z_beta)^2 * p*(1 - p) / (p - p0)^2
```

One of the nice things about sample size formulae is that we can use them to produce tables of minimum sample sizes. **Table 1** shows the minimum sample size required to achieve 80% power under different assumptions about the prevalence of N86Y mutations:

```{r}
data.frame(p0 = 0.15,
           p1 = seq(0.2, 0.5, 0.05),
           z_alpha = 1.96,
           z_beta = qnorm(0.8)) |>
  mutate(N = ceiling((z_alpha + z_beta)^2 * p1*(1 - p1) / (p1 - p0)^2),
         p1_percent = sprintf("%s%%", p1*100)) |>
  select(p1_percent, N) |>
  kable("html", col.names = c("Assumed present-day prevalence of N86Y mutation", "Sample size"), escape = FALSE) |>
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) |>
  column_spec(1, width = "6cm") |>
  column_spec(2, width = "4cm") |>
  footnote(general = "\nTable 1: minimum sample sizes under various assumptions about present-day prevalence", general_title = "")
```

Tables like this allow us to scan across values and work out what sample size is needed under different statistical assumptions, and to cross-reference this against what is achievable.

<div style="padding: 10px; border-radius: 5px; background-color: #fef3e7;">
  <span style="font-size: 1.2em; color: #d19554;">
    <i class="fas fa-comment"></i> Reflection:
  </span> 

  <span style="color:#d19554;"> 
  What sample size would you opt for in this case? What factors could help you decide on a reasonable assumption for the present day prevalence of N86Y mutations? What factors could help constrain the sample sizes that are feasible?
  </span>
</div>

## Detecting rare variants

Building on the success of your N86Y study in Gombe city, you have been invited to conduct a new study focused on identifying *pfk13* mutations in the nearby town of Pindiga. This study will target WHO validated mutations that are known to be associated with partial resistance to artemisinin. Instead of estimating the *prevalence* of these mutations, your goal is simply to determine whether any of these mutations are *present* in the population.

Your plan is to test individuals who present with malaria symptoms at a local health facility. For those who test positive for malaria, dried blood spots will be collected and subsequently sent for sequencing. However, due to limited resources, you are only able to sequence 100 samples.

The question, is: **with a sample size of only 100, is it worthwhile to pursue this study?**

### Framing the problem as a hypothesis test

This type of detection study can be framed as a null hypothesis test. The null hypothesis is that *there are no WHO-validated mutations present in the population*. In other words, the prevalence of these mutations is zero. Even a single observation of a WHO-validated mutant would disprove this null hypothesis. Therefore, unlike most statistical tests, there is no test statistic to calculate here. Instead, we simply reject the null hypothesis if we see a single sample containing a WHO-validated mutant.

It is straightforward to calculate power under this test. We start by assuming a known prevalence, $p$, for the validated mutations. We can derive the power in the following steps:

**Step 1: Chance of a single negative result:**

The probability that a single sample is negative (i.e. does not carry a validated *pfk13* mutation) is given by:
$$
\text{Pr}(\text{Negative}) = 1 - p
$$
**Step 2: Chance of two negative results:**

The probability of two negative samples is the probability of one negative sample multiplied by the probability of another negative sample:
$$
\begin{align}
\text{Pr}(\text{Two negatives}) &= (1 - p)(1 - p) \\
&= (1 - p)^2
\end{align}
$$

**Step 3: Chance of $n$ negative results:**

The probability that all $n$ samples are negative is the probability of one negative sample raised to the power $n$:
$$
\text{Pr}(n \text{ negatives}) = (1 - p)^n
$$
This assumes that samples are drawn independently from a much larger population.

**Step 4: Chance of at least one positive result:**

The chance of seeing at least one positive sample is equal to one minus the probability of seeing no positive samples. If we see a positive sample then we reject the null hypothesis. Hence, this is also our power:

$$
\text{Power} = 1 - (1 - p)^n
$$
This very simple expression can be used to guide our study design.

Complete the following R code to implement this formula. What is our power if we assume a prevalence of 5%?

```{r detection-1, exercise=TRUE}
# define parameters
p <- 
n <- 

# calculate power
power <- 

print(power)
```

```{r detection-1-solution}
# define parameters
p <- 0.05
n <- 100

# calculate power
power <- 1 - (1 - p)^n

print(power)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $\text{Power} = 0.994$ if we assume a prevalence of $p = 0.05$.

</details>
</br>

```{r presence-power-quiz}
quiz(caption = "QUIZ - Interpreting power",
     question("Based on this result, which of these statements is correct?",
           allow_retry = TRUE,
           answer("We have insufficient power under the current design", correct = FALSE),
           answer("We have adequate power under the current design", correct = FALSE),
           answer("We are over-powered under the current design", correct = TRUE),
           correct = "That is correct! We typically aim for at least 80% power, and we are way over this limit at 99.4% power.", 
           incorrect = "Hint: Remember what power we normally aim for in definitive (non-pilot) studies."
  )
)
```

Based on this result, we are over-powered to detect *pfk13* mutants. This gives us the freedom to reduce the sample size. So, how many samples are needed? Rearranging our power formula in terms of $n$ we obtain:
$$
n = \frac{\text{log}(1 - \text{Power})}{\text{log}(1 - p)}
$$

Complete the following R code to implement this sample size formula. What minimum sample size is needed if we assume a prevalence of 5% and we are aiming for 80% power?

```{r detection-2, exercise=TRUE}
# define prevalence
p <- 

# calculate minimum sample size
n <- 

print(n)
```

```{r detection-2-solution}
# define prevalence
p <- 0.05

# calculate minimum sample size
n <- log(1 - 0.8) / log(1 - p)

print(n)
```

<details>
<summary style="text-decoration: underline; color: red;">
`r fontawesome::fa("check", fill = "red")`Click to see the answer
</summary>

You should find that $n = 31.38$, which would be rounded up to $n = 32$.

</details>
</br>

This is a very promising result - we can definitely run a well-powered study within our resource constraints! However, we did make the fairly pessimistic assumption that validated *pfk13* mutants are at 5% prevalence in the population. In reality, we may want to catch them before they reach 5% in order to take pre-emptive measures. Again, we can consult a sample size table (**Table 2**):

```{r}
data.frame(p = c(0.001, 0.005, seq(0.01, 0.05, 0.01))) |>
  mutate(N = ceiling(log(1 - 0.8) / log(1 - p)),
         p_percent = sprintf("%s%%", p*100)) |>
  select(p_percent, N) |>
  kable("html", col.names = c("Assumed prevalence of pfk13 mutations", "Sample size"), escape = FALSE) |>
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) |>
  column_spec(1, width = "5cm") |>
  column_spec(2, width = "4cm") |>
  footnote(general = "\nTable 2: minimum sample sizes needed to achieve 80% power under various assumptions about pfk13 mutation prevalence", general_title = "")
```

<div style="padding: 10px; border-radius: 5px; background-color: #fef3e7;">
  <span style="font-size: 1.2em; color: #d19554;">
    <i class="fas fa-comment"></i> Reflection:
  </span> 

  <span style="color:#d19554;"> 
  Based on the values in **Table 2**, do you think it is worthwhile to conduct this study? If you ran a study that was powered down to 2% prevalence (80 samples) and did not find any *pfk13* mutants, would you be reassured by this result?
  </span>
</div>
