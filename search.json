[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This workshop is taking place before the 2024 ASTMH conference in New Orleans, USA.\nüìç Where: Aloft New Orleans, 225 Baronne St, New Orleans, USA\nüóìÔ∏è When: 11-12 Nov 2024, immediately prior to the ASTMH conference"
  },
  {
    "objectID": "schedule.html#day-1",
    "href": "schedule.html#day-1",
    "title": "Schedule",
    "section": "Day 1",
    "text": "Day 1\n\n\n\nTime\nTitle\nFormat\n\n\n\n\n12:00-12:30\nLunch\nIn meeting room\n\n\n12:30-1:00\nIntro: Goal of MMS\nLecture\n\n\n1:00-1:45\nIntroductions\n\n\n\n1:45-2:15\nModule 1: Sampling from a population\nLecture\n\n\n2:15-3:00\nModule 1: Practical\nLearnR tutorial\n\n\n3:00-3:30\nBreak\n\n\n\n3:30-4:00\nModule 2: Sample size based on precision\nLearnR tutorial\n\n\n4:00-4:45\nModule 2: Practical\nLearnR tutorial\n\n\n4:45:5:00\nBreak\n\n\n\n5:00-6:00\nDiscussion groups\nStructured discussion\n\n\n6pm onwards\nGroup dinner"
  },
  {
    "objectID": "schedule.html#day-2",
    "href": "schedule.html#day-2",
    "title": "Schedule",
    "section": "Day 2",
    "text": "Day 2\n\n\n\n\n\n\n\n\nTime\nTitle\nFormat\n\n\n\n\n8:30-9:00\nBreakfast\nIn meeting room\n\n\n9:00-9:05\nRecap of Day 1\n\n\n\n9:05-9:35\nGuest lecture: Logistics and challenges of designing a MMS study\nLecture\n\n\n9:35-10:05\nModule 3: Hypothesis testing, power and sample size calculation\nLecture\n\n\n10:05-10:50\nModule 3: Practical\nLearnR tutorial\n\n\n10:50-11:10\nBreak\n\n\n\n11:10-11:40\nModule 4: Intra-cluster correlation\nLecture\n\n\n11:40-12:30\nModule 4: Practical\nLearnR tutorial\n\n\n12:30-1:30\nLunch\n\n\n\n1:30-2:00\nModule 5: The DRpower tool\nLecture\n\n\n2:00-2:45\nModule 5: Practical\nLearnR tutorial\n\n\n2:45-3:00\nBreak\n\n\n\n3:00-3:15\nModule 6: Designing studies for multiple end-points\nLecture\n\n\n3:15-4:45\nModule 6: Practical\nLearnR tutorial\n\n\n4:45-5:30\nDiscussion and wrap-up"
  },
  {
    "objectID": "tutorials/module5/module5_DRpower.html",
    "href": "tutorials/module5/module5_DRpower.html",
    "title": "Module 5: The DRpower Tool",
    "section": "",
    "text": "This activity demonstrates the use of the DRpower R package and the accompanying web-based tool. In this activity you will learn:\n\nHow to design a multi-cluster pfhrp2/3 deletion study.\nHow to analyse and interpret the results of a pfhrp2/3 deletion study.\nHow to account for intra-cluster correlation in other study designs, such as prevalence surveys and presence/absence studies.\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module5/module5_DRpower.html#learning-outcomes",
    "href": "tutorials/module5/module5_DRpower.html#learning-outcomes",
    "title": "Module 5: The DRpower Tool",
    "section": "",
    "text": "This activity demonstrates the use of the DRpower R package and the accompanying web-based tool. In this activity you will learn:\n\nHow to design a multi-cluster pfhrp2/3 deletion study.\nHow to analyse and interpret the results of a pfhrp2/3 deletion study.\nHow to account for intra-cluster correlation in other study designs, such as prevalence surveys and presence/absence studies.\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module5/module5_DRpower.html#designing-a-multi-cluster-pfhrp23-deletion-study-in-ethiopia",
    "href": "tutorials/module5/module5_DRpower.html#designing-a-multi-cluster-pfhrp23-deletion-study-in-ethiopia",
    "title": "Module 5: The DRpower Tool",
    "section": "Designing a multi-cluster pfhrp2/3 deletion study in Ethiopia",
    "text": "Designing a multi-cluster pfhrp2/3 deletion study in Ethiopia\n\nBackground\nYou have been tasked by the Ethiopian National Malaria Control Programme (NMCP) to assist with design and implementation of a study into pfhrp2/3 deletions. Following the WHO 2020 recommendation, this study will be multi-site and conducted at a regional level. It will establish whether the prevalence of pfhrp2 deletions among clinical malaria cases is significantly above the 5% threshold. If so, this will trigger a nationwide switch in rapid diagnostic tests (RDTs) away from tests that rely exclusively on the HRP2 protein.\nA full study of this sort would normally be run in parallel over multiple regions, however, for the purposes of this tutorial we will focus exclusively on the Amhara region.\n\n\nUsing sample size tables\nFirst, we need to scope out roughly how many sites and how many samples are required. The easiest way to do this is via pre-calculated sample size tables. Go to the pfhrp2/3 Planner and navigate to the Explore tab. You should see a sample size table with the number of sites (health facilities) in rows, and the assumed prevalence of pfhrp2/3 deletions in columns.\nIn any power analysis, we must begin by assuming a known effect size under the alternative hypothesis. For this study, that means selecting an assumed prevalence of pfhrp2/3 deletions in the population. This assumption can be challenging and may feel somewhat arbitrary. It‚Äôs important to remember that as the assumed prevalence approaches the 5% detection threshold, the required sample size increases significantly. Conversely, assuming a very high prevalence may reduce the public health relevance of the study by focusing power on detecting only large shifts in the parasite population‚Äîchanges that could already be affecting clinical diagnosis.\nTo balance these factors, we recommend a default assumed prevalence of 10%. This value provides a reasonable sensitivity level while keeping sample sizes manageable. For more insights on selecting this parameter, refer to the FAQ section in the web-based pfhrp2/3 Planner.\nQuestion: What is the total sample size needed (over all clusters) if we recruit 5 clusters?\nOptions:\n\n2480 (correct)\n678\n408\n300\n\nQuestion: What about if we recruit 10 clusters?\nOptions:\n\n2480\n678\n408\n300 (correct)\n\nAs the number of sites recruited increases, the total sample size needed actually decreases. This may seem counterintuitive, as we might expect that adding more clusters would inherently make the study larger. What‚Äôs happening here is that with more clusters, the samples become more independent. By sampling from a greater number of distinct sites, we reduce the likelihood of repeatedly sampling from the same sub-population, thereby minimizing the impact of intra-cluster correlation. As a result, fewer samples are needed overall to achieve the same level of statistical power.\n\n\nFactoring in financial considerations\nThe Ethiopian NMCP has provided you with rough estimates of the costs of various aspects of the planned study, including the costs of RDT testing, microscopy, staff time, transport of samples and training of staff. They estimate it will cost $500 USD per cluster recruited, plus an additional $6 USD per sample enrolled.\nQuestion: Based on these financial figures, and using the numbers from the sample size table, how many clusters should you recruit to minimize the total cost?\nOptions:\n\n2\n5\n7 (correct)\n10\n\nThe design with 7 clusters comes in at $6,365 UDS total. This is made up of $3,500 for setting up the clusters plus $2,856 for the samples enrolled. This is the cheapest of the possible options, although between 6 and 10 clusters are quite similar total costs and may also be viable options.\n\n\nSelecting sites\nNext, we have to choose how we will select the 7 sites. Common choices are 1) sentinel site surveillance, where known and established sites are chosen based on feasibility, and 2) random selection, where a complete list of all health facilities is compiled and a defined number of sites is drawn at random from this list.\nSome advantages of sentinel site surveillance include:\n\nConsistency and Comparability Over Time: Sentinel sites are typically chosen for long-term monitoring, enabling consistent data collection over time. This allows for more precise trend analysis and easier comparisons of changes in disease prevalence.\nCost-Effectiveness and Established Infrastructure: Sentinel sites are often strategically chosen for accessibility and logistical ease, making them more cost-effective than random site selection, which may require travel to remote or difficult-to-reach locations. Additionally, these sites usually have established infrastructure, trained personnel, and local relationships, streamlining data collection and improving data quality..\nTargeting High-Burden or Priority Areas: Sentinel sites are often selected based on epidemiological significance, such as higher disease burden or strategic relevance for outbreak monitoring. This targeted approach can be advantageous in understanding and responding to disease patterns in high-risk areas, which might be overlooked in a random selection approach.\n\nOn the other hand, there are some advantages of random site selection:\n\nImproved Representativeness and Reduced Selection Bias: Random site selection from a complete list provides a more representative sample of the population and minimizes biases associated with known sentinel sites, which are often chosen based on specific criteria like accessibility or historical disease burden. This approach enhances the validity and generalizability of prevalence estimates, making them more applicable across various areas, including those not included in the survey.\nGreater Diversity of Environmental Conditions: Randomly selected sites are more likely to capture a range of environmental, socio-economic, and health system conditions. This variation is crucial for understanding factors that influence disease prevalence and for developing interventions that can apply across diverse settings.\n\nQuestion: Which of these designs would you choose for this study?\nOptions:\n\nSentinel site surveillance (both answers correct)\nRandom sampling (both answers correct)\n\nFor the sake of this tutorial we will go with random selection of sites. A complete list of all health facilities in the Amhara region is compiled, and 7 sites are selected from this list at random. Table 1 gives the chosen sites and the target sample size based on our initial calculations.\n\n\n\n\n\n\n\n\n\nSite name\nSample size\n\n\n\n\nDese\n68\n\n\nMotta\n68\n\n\nDebark\n68\n\n\nMetema\n68\n\n\nChagni\n68\n\n\nFinote Selam\n68\n\n\nAlem Ketema\n68\n\n\n\n\nTable 1: target sample sizes based on initial exploration\n\n\n\n\n\n\n\n\n\n\n\nRefining sample sizes\nWith a preliminary study plan in place, we can now incorporate some real-world constraints. After discussions with site leads at each of the seven locations, we‚Äôve learned that several sites may face challenges in reaching the target sample size. This could stem from factors like limited staffing, low malaria incidence, or other local issues. To address this, you‚Äôve adjusted the sample sizes by increasing recruitment targets at sites where it is feasible to do so. Here‚Äôs the updated sample size table:\n\n\n\n\n\n\n\n\n\nSite name\nSample size\n\n\n\n\nDese\n100\n\n\nMotta\n30\n\n\nDebark\n70\n\n\nMetema\n55\n\n\nChagni\n90\n\n\nFinote Selam\n70\n\n\nAlem Ketema\n60\n\n\n\n\nTable 2: target sample sizes factoring in constraints\n\n\n\n\n\n\n\n\n\nBut now we are faced with a problem - how do we know if this new design still has adequate power? We can no longer rely on sample size tables, which assume the same sample size per cluster. On the other hand, we cannot simply assume that power will be adequate. This is where the Design tab of the pfhrp2/3 Planner becomes useful.\nDownload a .csv file with the current study plan from here. You can then upload this spreadsheet into the we app using the ‚ÄúUpload a .csv file‚Äù option. You should find that the table populates with these values.\nYou‚Äôll notice that drop-out is also considered at this stage. We are assuming 10% drop-out in each site, but for a more rigorous approach you may want to factor in different drop-out levels by site. When you click ‚ÄúCalculate adjusted sample sizes‚Äù you should see a table that contains the adjusted sample size, buffered for drop-out.\nNow we are ready to estimate power. On the same page you will find drop-down menus where you can set the assumed prevalence of pfhrp2/3 deletions and the intra-cluster correlation. When you git the ‚ÄúEstimate power‚Äù button the tool will run a simulation-based power analysis (this may take a few seconds). Because this result is simulation-based we only get an estimate of the power, and not an exact figure. If you want a more precise estimate you can try increasing the number of simulations, although this will take longer to run.\nQuestion: Do you think power is adequate under the current plan?\nOptions:\n\nYes (correct)\nNo\n\nRunning an estimate with 1000 simulations should show that the study‚Äôs power is close to 80%, with the 95% confidence interval likely spanning this target. For future reference, you can download a copy of your power analysis from the ‚ÄúGenerate report‚Äù tab. Based on these results, the study plan appears statistically sound and ready for consideration. However, final approval should also take into account logistical and financial factors to ensure feasibility."
  },
  {
    "objectID": "tutorials/module5/module5_DRpower.html#analyzing-pfhrp23-data",
    "href": "tutorials/module5/module5_DRpower.html#analyzing-pfhrp23-data",
    "title": "Module 5: The DRpower Tool",
    "section": "Analyzing pfhrp2/3 data",
    "text": "Analyzing pfhrp2/3 data\nSome time later, the Ethiopian NMCP request your assistance once again to analyse the results of the pfhrp2/3 deletion study, which was carried out according to your plan. The results of the study are shown in Table 3.\n\n\n\n\n\n\n\n\n\n\nSite name\npfhrp2 negative\nSample size\n\n\n\n\nDese\n10\n100\n\n\nMotta\n1\n30\n\n\nDebark\n9\n70\n\n\nMetema\n7\n55\n\n\nChagni\n2\n90\n\n\nFinote Selam\n2\n70\n\n\nAlem Ketema\n8\n60\n\n\n\n\nTable 3: results of the pfhrp2/3 study\n\n\n\n\n\n\n\n\n\n\nYou can download these results in .csv form here. The easiest way to analyse these data is via the Analysis tab of the pfhrp2/3 Planner. Select ‚ÄúUpload a .csv file‚Äù and import the data.\nNow click the ‚ÄúEstimate prevalence‚Äù button to perform a Bayesian analysis of the data. This analysis takes into account the intra-cluster correlation in the data, which is estimated and accounted for in the prevalence estimate. Therefore there is no need to estimate a design effect or effective sample size as this is accounted for automatically.\nYou will also see a ‚ÄúProbability above threshold‚Äù field. This tells us the probability that the prevalence is above the 5% threshold.\nQuestion: Which of these is true:\nOptions:\n\nThere is less than 95% probability that the prevalence is above the 5% threshold. Therefore we conclude that the prevalence is below 5%.\nThere is more than 95% probability that the prevalence is above the 5% threshold. Therefore we conclude that the prevalence is above 5%. (correct)\n\nIn this example, we have sufficient evidence to conclude that the prevalence of pfhrp2/3 deletions in the Amhara region is above the 5% threshold. Therefore, according to the WHO 2020 recommendation, we would be justified in switching RDTs to a brand that does not rely exclusively on the HRP2 protein.\n\nEstimating the ICC\nAs noted above, the DRpower method automatically accounts for ICC when estimating prevalence, so a separate ICC estimate is not strictly necessary. However, obtaining an ICC estimate can still be valuable for future study planning. It‚Äôs a straightforward process and contributes to a broader understanding of ICC levels, which can benefit researchers planning similar studies in the future or in neighbouring regions.\nStill on the Analysis tab, scroll down to the ‚ÄúEstimate ICC‚Äù button. Clicking this button gives an estimate of the ICC, including a point estimate and a 95% CrI.\nQuestion: Based on the ICC analysis, which of these is true:\nOptions:\n\nThere is strong evidence that the ICC is below 0.2 **(correct)*\nThere is strong evidence that the ICC is below 0.1\nThere is strong evidence that the ICC is below 0.05\nThere is strong evidence that the ICC is zero (complete independence)\n\nWe can navigate to the ‚ÄúGenerate report‚Äù tab to download a report detailing our analysis.\n\n\nRecap key points\nOur pfhrp2/3 analysis is now complete. We began with a well-powered study design, carefully crafted to be robust and realistic. By accounting for varying sample sizes across sites and anticipating potential drop-out, we reinforced the reliability of our approach. Beyond statistical considerations, we incorporated logistical and financial constraints to ensure the study‚Äôs feasibility. Importantly, we accounted for possible over-dispersion at both the design and analysis stages, increasing our confidence in the results. Ultimately, this rigorous approach allows us to conclude with confidence that the prevalence of pfhrp2/3 deletions exceeds the 5% threshold‚Äîa finding with significant implications for malaria control at the national level."
  },
  {
    "objectID": "tutorials/module5/module5_DRpower.html#bonus-questions",
    "href": "tutorials/module5/module5_DRpower.html#bonus-questions",
    "title": "Module 5: The DRpower Tool",
    "section": "Bonus questions",
    "text": "Bonus questions\nThe DRpower R package is primarily designed with pfhrp2/3 studies in mind, but also contains some functions that are useful for other multi-cluster designs. For example, it can be used when estimating prevalence to within a specified margin of error, or when detecting the presence of rare variants, while taking into account intra-cluster correlation. These features are only present in the R package, and are not present on the web-based pfhrp2/3 Planner.\nThe following R code calculates the sample size needed for a multi-cluster prevalence survey to reach a target margin of error. Try varying the value of the ICC and see how this impacts sample size.\n\n# calculate sample size needed to achieve a target MOE\nDRpower::get_sample_size_margin(MOE = 0.1, n_clust = 10, prevalence = 0.2, ICC = 0.01)\n\n[1] 7\n\n\nUntil now, we have focused on using the Wald interval to construct a 95% confidence interval (CI). Although the Wald interval is straightforward to calculate, it relies on assumptions that may not always hold. For instance, with a small sample size or when prevalence is near 0 or 1, the Wald interval can extend beyond logical boundaries, producing values below 0 or above 1. Such results are nonsensical and indicate that the interval may not accurately represent the underlying data.\nAn alternative approach is to use the Clopper-Pearson (CP) interval. Unlike the Wald interval, the CP interval remains within the 0 to 1 range, making it more statistically robust. However, this robustness comes with a trade-off: the CP interval tends to be conservative, resulting in wider intervals.\nPlay around with the following R code, which still aims to reach a target margin of error but now assuming CP intervals. What do you notice about the sample size compared to the simple Wald interval approach?\n\n# calculate sample size needed to achieve a target MOE using CP intervals\nDRpower::get_sample_size_margin_CP(MOE = 0.1, n_clust = 10, prevalence = 0.2, ICC = 0.01)\n\n[1] 10\n\n\nFinally, we may want to look for the presence of rare variants in a multi-cluster design while accounting for intra-cluster correlation.\nPlay around with the following code. What total sample size would you need if you assume 10 clusters, 1% prevalence of the variant of interest, and an ICC of 0.05? How would this compare to the total sample size if you treated all sites as independent?\n\n# calculate sample size needed in a multi-cluster presence/absence study\nDRpower::get_sample_size_presence(n_clust = 5, prevalence = 0.05, ICC = 0.05)\n\n[1] 8"
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "",
    "text": "This activity focuses on using margin of error arguments to come up with an appropriate sample size for a hypothetical study. In this activity you will learn:\n\nHow to calculate a 95% confidence interval from prevalence data\nHow to derive the formula for sample size directly from the confidence interval formula\nHow to calculate a minimum sample size using assumptions about prevalence and margin of error\nHow to account for drop-out and positive fraction\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html#learning-outcomes",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html#learning-outcomes",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "",
    "text": "This activity focuses on using margin of error arguments to come up with an appropriate sample size for a hypothetical study. In this activity you will learn:\n\nHow to calculate a 95% confidence interval from prevalence data\nHow to derive the formula for sample size directly from the confidence interval formula\nHow to calculate a minimum sample size using assumptions about prevalence and margin of error\nHow to account for drop-out and positive fraction\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html#background",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html#background",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Background",
    "text": "Background\nYou have been recruited by the National Malaria Control Programme (NMCP) of the Democratic Republic of the Congo (DRC) to assist with study design. The NMCP is concerned about the potential spread of mutations in the parasite population that confer partial resistance to the drug combination Sulfadoxine-Pyrimethamine (SP). The dhps K540E mutation, which is known to be associated with high level SP resistance alongside other common mutations, has recently been found at high prevalence (72%) in neighbouring Uganda. In the last few weeks there have been anecdotal reports of SP failure in Rutshuru town, which lies in Eastern DRC close to the border with Uganda, hence the NMCP is concerned about possible flow of drug resistant parasites over the border.\nThe NMCP plans to conduct a cross-sectional study to estimate the prevalence of the dhps K540E mutation within Rutshuru town. Your job is to work out the appropriate sample size for this study."
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html#results-of-a-pilot-study",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html#results-of-a-pilot-study",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Results of a pilot study",
    "text": "Results of a pilot study\nThankfully, a pilot study has already been conducted in Rutshuru. This pilot study included 100 participants, chosen at random from households within the town, who were tested for malaria via rapid diagnostic test (RDT). 23 people tested positive for malaria and these samples were sent away for genetic sequencing. 19 samples were successfully sequenced, of which 5 were positive for the K540E mutation.\nQuestion: Which is the correct equation when calculating the prevalence of K540E mutations from the pilot data?\nOptions:\n\n23 / 100\n19 / 100\n5 / 23\n5 / 19 (correct)\n\nRecall that we can use the following formula to calculate a 95% confidence interval on our prevalence estimate:\n\\[\n\\hat{p} \\pm z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}\n\\] Complete the following R code by entering the appropriate values for p and N to compute this interval:\n\n# enter values for the estimated prevalence and the sample size\np &lt;- 5 / 19 # NB, delete value\nN &lt;- 19     # NB, delete value\n\n# calculate the margin of error (MOE)\nz &lt;- qnorm(0.975)\nMOE &lt;- z*sqrt(p*(1 - p) / N)\n\n# report the point estimate and 95% confidence interval\ndata.frame(prevalence = p,\n           lower = p - MOE,\n           upper = p + MOE)\n\n  prevalence     lower     upper\n1  0.2631579 0.0651572 0.4611586\n\n\nQuestion: Which of these statements is correct:\nOptions:\n\nThe 95% CI ranges from 1.5% to 80.2%\nThe 95% CI ranges from 6.5% to 46.1% (correct)\nThe 95% CI ranges from 24.0% to 28.0%\nThe 95% CI ranges from 0.065% to 0.461%\n\nThe 95% CI shows that we are very uncertain of the prevalence of K540E mutations based on our pilot data alone. While our best estimate of the prevalence is 26%, the plausible range includes anything from 6.5% to 46.1%. The NMCP considers this range too wide to be useful. Hence, they plan to conduct a follow-up study."
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html#calculating-the-sample-size",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html#calculating-the-sample-size",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Calculating the sample size",
    "text": "Calculating the sample size\nOur first task is to convert the formula for the margin of error (MOE) into a new formula that tells us the appropriate sample size. If you are comfortable with the mathematics and want to see how to do this then follow these steps. If not, you can jump ahead to Step 3 to see the final formula.\nStep 1: Write down the formula for the MOE\nWe will use the mathematical symbol \\(m\\) for the MOE:\n\\[\nm = z_{1-\\alpha/2}\\sqrt{\\frac{p(1-p)}{N}}\n\\] Step 2: Square both sides\n\\[\nm^2 = z_{1-\\alpha/2}^2 \\frac{p(1-p)}{N}\n\\] Step 3: Multiply by \\(N\\) and divide by \\(m^2\\)\n\\[\nN = z_{1-\\alpha/2}^2 \\frac{p(1-p)}{m^2}\n\\] We can use this formula to tell us the appropriate sample size given an assumed value of the prevalence (\\(p\\)) and a target value for the MOE (\\(m\\)).\nQuestion: What value should we assume for \\(p\\)?\nOptions: (all three answers are correct as they are all reasonable)\n\n0.26 based on the pilot data\n0.5 as this is the most pessimistic assumption in terms of requiring the largest sample size\n0.72 based on the prevalence in neighbouring Uganda\n\nAll three answers above are reasonable as long as they can be justified. But for the sake of argument let‚Äôs assume a value of \\(p=0.26\\). The NMCP has decided that a MOE of 5% is acceptable. Complete the following R code by entering the appropriate values for p and m to compute the resulting sample size:\n\n# enter values for the estimated prevalence and the sample size\np &lt;- 0.26     # NB, delete value\nm &lt;- 0.05     # NB, delete value\n\n# calculate the raw sample size\nz &lt;- qnorm(0.975)\nN &lt;- z^2*p*(1 - p) / m^2\n\nprint(N)\n\n[1] 295.6387\n\n\nYou should have obtained a sample size of 295.64, which we would round up to \\(N=296\\) to give a whole number. The nice thing about this calculation is that we can double check that it is correct. Try entering the values \\(p=0.26\\) and \\(N=296\\) into the 95% CI formula that we used in the pilot data analysis. If our calculations were correct, you should find that the resulting MOE is very close to 5%."
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html#buffering",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html#buffering",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Buffering",
    "text": "Buffering\nBuffering refers to increasing a sample size to allow for events that are out of our control and that can result in drop-out (loss of samples). Some ways that drop-out can occur are through:\n\nParticipants withdrawing consent\nParticipants dying or leaving the area\nSamples being lost during transportation\nSamples becoming contaminated\nSamples failing amplification or sequencing resulting in a lack of genetic data\nData being lost due to computational errors\n\nWe cannot completely eliminate the risk of drop-out, but by buffering sample sizes we can be robust to it. If we expect a proportion \\(d\\) of samples to be lost, then the formula for adjusted sample size is:\n\\[\nN_{\\text{buffered}} = \\frac{N_{\\text{original}}}{1 - d}\n\\]\nThrough consulting with lab technicians and the study team, you estimate that 10% of samples may be lost to drop-out. Complete the following R code to come up with a buffered sample size:\n\n# enter value for estimated dropout\nr &lt;- 0.1     # NB, delete value\n\n# calculate the buffered sample size\nN_buffered &lt;- N / (1 - r)\n\nprint(N_buffered)\n\n[1] 328.4874\n\n\nQuestion: What is the correct buffered sample size?\nOptions:\n\n300\n328\n350\n329 (correct)"
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html#accounting-for-positive-fraction",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html#accounting-for-positive-fraction",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Accounting for positive fraction",
    "text": "Accounting for positive fraction\nSo far, we have focused on working out how many confirmed malaria cases we need in our study. However, recall that this will be a cross-sectional study with individuals being sampled at random from households within Rutshuru town. Many of the individuals tested will not be positive for malaria. It may be useful for us to know how many individuals we need to test as part of this study, which may be considerably higher than the number of confirmed malaria cases.\nThe NMCP estimates that 25% of the population of Rutshuru will be positive for malaria by RDT. We can use the same buffering formula as before, but now using the positive fraction (\\(f\\)) to inflate our sample size:\n\\[\nN_{\\text{test}} = \\frac{N_{\\text{confirmed}}}{f}\n\\]\nComplete the following R code to work out the number of people we will need to test to achieve the final target sample size:\n\n# enter value for estimated positive fraction\nN_buffered &lt;- 329\nf &lt;- 0.25     # NB, delete value\n\n# calculate the testing sample size\nN_test &lt;- N_buffered / f\n\nprint(N_test)\n\n[1] 1316\n\n\nQuestion: How many people will we need to test to obtain the target sample size of 329 positive malaria cases?\nOptions:\n\n534\n923\n1316 (correct)\n1536\n\nYou have now completed your study design exercise. Your recommendation to the NMCP is as follows:\nAssuming a prevalence of K540E mutations of 26% based on pilot data, a sample size of 329 confirmed malaria cases will be needed to estimate prevalence to within 5% margin of error. This number is buffered to take into account an assumed 10% drop-out. Assuming that malaria prevalence is 25% by RDT in Rutshuru town, this translates to 1316 individuals who will need to be tested in the cross-sectional study design."
  },
  {
    "objectID": "tutorials/module2/module2_sample_size_from_MOE.html#bonus-questions",
    "href": "tutorials/module2/module2_sample_size_from_MOE.html#bonus-questions",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Bonus questions",
    "text": "Bonus questions\nThe study design above is based on strong statistical principles. However, it is worth testing how robust these numbers are to changes in our assumptions.\n\nUnder the chosen sample size of 296 (after drop-out), what would be your margin of error under the worst case scenario that the true prevalence of the K540E mutation was actually 50%?\nWe estimated that 1316 people will need to be tested based on an assumed 25% prevalence of malaria. But what if malaria prevalence is actually only 15% in Rutshuru town?"
  },
  {
    "objectID": "tutorials/module4/module4_ICC.html",
    "href": "tutorials/module4/module4_ICC.html",
    "title": "Module 4: Intra-cluster correlation",
    "section": "",
    "text": "This activity focuses on measuring over-dispersion in multi-cluster studies using the example of a hypothetical pfhrp2/3 deletion study in Tanzania. In this activity you will learn:\n\nWhat over-dispersion looks like in prevalence data, and how to detect it statistically\nHow to measure over-dispersion using different metrics\nThe impact of over-dispersion on statistical efficiency\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module4/module4_ICC.html#learning-outcomes",
    "href": "tutorials/module4/module4_ICC.html#learning-outcomes",
    "title": "Module 4: Intra-cluster correlation",
    "section": "",
    "text": "This activity focuses on measuring over-dispersion in multi-cluster studies using the example of a hypothetical pfhrp2/3 deletion study in Tanzania. In this activity you will learn:\n\nWhat over-dispersion looks like in prevalence data, and how to detect it statistically\nHow to measure over-dispersion using different metrics\nThe impact of over-dispersion on statistical efficiency\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module4/module4_ICC.html#analyzing-data-from-a-multi-cluster-pfhrp2-deletion-study",
    "href": "tutorials/module4/module4_ICC.html#analyzing-data-from-a-multi-cluster-pfhrp2-deletion-study",
    "title": "Module 4: Intra-cluster correlation",
    "section": "Analyzing data from a multi-cluster pfhrp2 deletion study",
    "text": "Analyzing data from a multi-cluster pfhrp2 deletion study\n\nBackground\nYou are collaborating with the Tanzanian National Malaria Control Programme (NMCP) to investigate the prevalence of pfhrp2/3 gene deletions in the Dodoma region of Tanzania. These gene deletions pose a significant threat to malaria control efforts as they can lead to parasites being undetectable by rapid diagnostic tests (RDTs) that rely exclusively on the HRP2 protein. Undetected cases may lead to delays in treatment or missed malaria diagnoses, undermining effective case management.\nA multi-cluster study has been performed in 8 sites within the Dodoma region of Tanzania. The results of this study are shown below:\n\n\n\n\n\n\n\n\n\n\n\nSite\nConfirmed Malaria\n(n)\npfhrp2 Deleted\n(x)\npfhrp2 Deletion Prevalence\n(p)\n\n\n\n\nChalinze\n100\n7\n0.070\n\n\nIbwaga\n80\n12\n0.150\n\n\nLukali\n100\n7\n0.070\n\n\nMalolo\n100\n26\n0.260\n\n\nMafene\n95\n25\n0.263\n\n\nMpendo\n100\n45\n0.450\n\n\nNhinhi\n50\n0\n0.000\n\n\nRudi\n100\n11\n0.110\n\n\n\n\nTable 1: pfhrp2 deletion data by Site\n\n\n\n\n\n\n\n\n\n\n\nThe columns of this table are available in R as the variables n, x and p.\n\n\nEstimating the global prevalence\nWe want to use the information over all 8 sites to estimate the prevalence of pfhrp2 deletions in the Dodoma region as a whole. One way to do this is to take the mean over sites:\n\nmean(p)\n\n[1] 0.171625\n\n\nThe mean prevalence is around 17%. However, this calculation ignores differences in sample sizes between sites. For example, the Nhinhi site is given just as much weight as the Rudi site, despite having half the number of confirmed malaria cases. A different approach is to use a weighted average, where the weights are given by the sample sizes:\n\n# define weights based on sample size\nweights &lt;- n\n\n# calculate prevalence as a weighted average over sites\nsum(weights * p) / sum(weights)\n\n[1] 0.1834276\n\n\nWe now find that prevalence is closer to 18%. In this example, there is not much difference between the two methods, but in general they can give quite different results. Neither approach is more correct than the other, they just have different strengths and weaknesses. The unweighted mean is more conservative when there could be large intra-cluster correlation. On the other hand, the weighted mean avoids the issue of small clusters having a large influence on the final estimate.\nFor the purposes of this tutorial, we will use the unweighted mean as our estimate over all sites. This value is often called the ‚Äúglobal‚Äù prevalence, and is given the symbol \\(\\hat{p}\\).\n\np_global &lt;- mean(p)\n\n\n\nDetecting over-dispersion\nNow that we have an estimate of the global prevalence, we can look for over-dispersion in the data. If all patients enrolled in the study have the same probability \\(\\hat{p}\\) of carrying the pfhrp2 deletion, then we expect to see a certain level of variation between sites. Most of the time, the site-level prevalence should be within the following 95% interval: \\[\n\\hat{p} \\pm z_{1 - \\alpha/2}\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n_i}}\n\\] where \\(n_i\\) is the sample size in the \\(i^{\\text{th}}\\) site. Because this is a 95% interval, we should expect sites to fall within this range 95% of the time. For our study involving 8 sites, it would be very unusual to see more than one site with a prevalence outside this range.\nThe plot below shows the site-level prevalence in red. The global mean prevalence of 17% shown as a horizontal dashed line and the 95% interval shown as an error bar.\n\n\n\n\n\nQuestion: How many sites have a prevalence outside the expected 95% interval?\nOptions:\n\n0\n2\n4\n6 (correct)\n\nQuestion: From this plot, does the prevalence appear to be over-dispersed, under-dispersed, or neither?\nOptions:\n\nOver-dispersed (correct)\nUnder-dispersed\nNeither\n\nQuestion: What could cause this over-dispersion?\nOptions:\n\nLimited migration and gene flow between sites.\nVariation in treatment practices leading to different selective pressures between sites.\nLocal outbreaks within sites, driven by infected individuals carrying pfhrp2 deletions.\nAll of the above (correct)\n\nQuestion: Why does the Nhinhi site have a slightly wider 95% interval than the other sites?\nOptions:\n\nThe Nhinhi site has higher prevalence of pfhrp2 deletions.\nThe diagnostic methods used at the Nhinhi site are less accurate.\nThe Nhinhi site is geographically isolated.\nThe sample size in Nhinhi is lower than other sites (correct)"
  },
  {
    "objectID": "tutorials/module4/module4_ICC.html#measuring-the-impact-of-over-dispersion",
    "href": "tutorials/module4/module4_ICC.html#measuring-the-impact-of-over-dispersion",
    "title": "Module 4: Intra-cluster correlation",
    "section": "Measuring the impact of over-dispersion",
    "text": "Measuring the impact of over-dispersion\n\nThe design effect\nOne way of quantifying the effect of over-dispersion is through the design effect (\\(D_{\\text{eff}}\\)). We can estimate the design effect by calculating the observed variance between sites and dividing this by the variance that we would expect to see under simple random sampling (SRS), which is when all individuals in the study are perfectly independent.\nThe observed variance between sites can be calculated as:\n\\[\ns^2 = \\frac{1}{c-1}\\sum_{i=1}^c (\\hat{p}_i - \\hat{p})^2\n\\] where \\(c\\) is the number of sites (8 in our case) and \\(\\hat{p}_i\\) is the prevalence in site \\(i\\). This formula is called the sample variance, and it is found in many areas of statistics. We don‚Äôt need to calculate this value by hand, instead we can do it very easily in R using the var() function:\n\n# calculate observed variance between sites\nvar_observed &lt;- var(p)\nprint(var_observed)\n\n[1] 0.02114684\n\n\nNext, we need to calculate the variance that we would expect to see under simple random sampling (SRS). This is given by:\n\\[\n\\text{var}_{\\text{SRS}} = \\frac{1}{c} \\sum_{i=1}^c \\frac{\\hat{p}(1 - \\hat{p})}{n_i}\n\\] We can calculate this in R as follows:\n\nvar_SRS &lt;- mean(p_global*(1 - p_global) / n)\nprint(var_SRS)\n\n[1] 0.001653192\n\n\nOur observed variance was 0.0211, but the variance we would expect to see under SRS is only 0.00165. This tells us that our data are more variable than we would expect by chance, in other words this confirms that there is over-dispersion in our data. We calculate the design effect as the ratio of these two quantities:\n\nDeff &lt;- var_observed / var_SRS\nprint(Deff)\n\n[1] 12.79152\n\n\nWe obtain a design effect of \\(D_{\\text{eff}}=12.79\\).\nQuestion: The design effect measures:\nOptions:\n\nHow good a study is.\nThe statistical inefficiency of a study, relative to SRS. (correct)\nThe number of sites used in a study.\nThe average prevalence across sites.\n\nThe design effect is a measure of the statistical inefficiency of a study design. Larger values indicate less efficient designs, with a value of \\(D_{\\text{eff}}=1\\) representing a gold standard in terms of statistical efficiency. Our observed value of \\(D_{\\text{eff}}=12.79\\) indicates that our study has a high level of statistical inefficiency due to the high level of over-dispersion in the data.\nQuestion: A high value of the design effect means we can expect to see:\nOptions:\n\nGreater uncertainty around our global estimate of the prevalence \\(\\hat{p}\\).\nLower power.\nLarger sample sizes needed.\nAll of the above. (correct)\n\n\n\nThe effective sample size\nAnother way to quantify the impact of over-dispersion is through the effective sample size, \\(N_{\\text{eff}}\\). The effective sample size tells us how many perfectly independent samples we would need in order to achieve the same level of statistical efficiency. In other words, it tells us how large our study would need to be if we could get rid of over-dispersion completely. We calculate \\(N_{\\text{eff}}\\) by dividing the true total sample size by the design effect:\n\\[\nN_{\\text{eff}} = \\frac{\\sum_{i=1}^c n_i}{D_{\\text{eff}}}\n\\] Here is R code that performs this calculation:\n\nsum(n) / Deff\n\n[1] 56.67818\n\n\nOur effective sample size is just 56.7, even though our total sample size was 725! Amazingly, this means that, due to correlations within sites, it‚Äôs as if we had enrolled only 56 patients! This emphasizes the importance of considering over-dispersion. If we ignored over-dispersion in our analysis then we might kid ourselves into thinking we had a large amount of data and hence very precise estimates of the regional prevalence. When we deal with it correctly we get much more robust results.\n\n\nThe intra-cluster correlation coefficient\nThe third and final measure that we will consider is the intra-cluster correlation coefficient (ICC), often given the symbol \\(\\rho\\). This is a measure between 0 and 1 that describes how correlated observations are within a cluster (site). The relationship between the ICC and the design effect is:\n\\[\nD_{\\text{eff}} = 1 + \\rho(\\bar{n} - 1)\n\\] where \\(\\bar{n}\\) is the mean sample size over sites. Notice that when there is zero intra-cluster correlation (\\(\\rho=0\\)) the design effect equals 1, indicating that we are highly statistically efficient. The larger the value of \\(\\rho\\), the higher \\(D_{\\text{eff}}\\) gets and the more inefficient our design.\nWe can flip this equation around to give us the ICC as a function of the design effect:\n\\[\n\\rho = \\frac{D_{\\text{eff}} - 1}{\\bar{n} - 1}\n\\]\nComplete the following R code to calculate the value of the ICC from the design effect:\n\nn_mean &lt;- mean(n)\n\n(Deff - 1) / (n_mean - 1)\n\n[1] 0.1315651\n\n\nBut why did we bother working out the ICC? What advantage does this measure have over other measures like the design effect? To answer this, consider the following question:\nQuestion: Another study run in Kenya with a much larger sample size also found a design effect of \\(D_{\\text{eff}} = 12.79\\). Does this mean the two countries have similar levels of intra-cluster correlation?\nOptions:\n\nYes, the design effect is the same so the ICC must also be the same.\nNo, the design effect depends on the same size and so we cannot make this comparison. (correct)\n\nThe same value of \\(\\rho\\) can give very different values of \\(D_{\\text{eff}}\\), depending on the sample size \\(\\bar{n}\\). For this reason, we should be very careful when comparing the design effect between studies. Just because one study found a small design effect, does not mean that we can also assume the same small value when designing a new study. On the other hand, the ICC does not depend on the sample size and so tends to be more useful when comparing between studies."
  },
  {
    "objectID": "tutorials/module4/module4_ICC.html#bonus-questions",
    "href": "tutorials/module4/module4_ICC.html#bonus-questions",
    "title": "Module 4: Intra-cluster correlation",
    "section": "Bonus questions",
    "text": "Bonus questions\nIn an earlier module, you learned how to calculate a confidence interval (CI) on a prevalence estimate using the Wald interval. Use the information in Table 1 to calculate a 95% CI around our global prevalence estimate. For the sample size, use the total sample size summed over all clusters. This is equivalent to assuming that all observations are completely independent. What do you get for the margin of error, and the lower and upper limits of your 95% CI?\nNow repeat this analysis, but instead of using the total sample size, use the effective sample size that you calculated above. This analysis does not assume that all observations are independent, and instead takes account of over-dispersion. What do you get for the margin of error, and the lower and upper limits of your 95% CI? How does this compare with your previous answer?"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "All of the materials from this workshop are freely available. We suggest you work through them in order (see the Schedule for the order in which materials were presented at the pre-ASTMH workshop)."
  },
  {
    "objectID": "tutorials.html#list-of-all-the-tutorials",
    "href": "tutorials.html#list-of-all-the-tutorials",
    "title": "Tutorials",
    "section": "List of all the tutorials",
    "text": "List of all the tutorials\n\nModule 1: Sampling from a population\nModule 2\nModule 3\nModule 4\nModule 5\nModule 5\nModule 6"
  },
  {
    "objectID": "index.html#workshop-format",
    "href": "index.html#workshop-format",
    "title": "Malaria Molecular Surveillance Study Design workshop",
    "section": "Workshop format",
    "text": "Workshop format\nOver the course of 1.5 days (see Schedule) we will have a combination of lectures, hands-on exercises, and group discussions. The workshop will be in English. We will cover several topics relevant to malaria molecular surveillance (MMS) study design, including:\n\nStatistical power\nSample size calculation\nEstimating prevalence\nMeasuring changes in prevalence\nCluster-based study design and intra-cluster correlation\nGuest lectures from experts conducting MMS studies"
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "Malaria Molecular Surveillance Study Design workshop",
    "section": "Materials",
    "text": "Materials\nAll of the materials from this workshop are freely available."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "All of the materials from this workshop are freely available. We suggest you work through them in order (see the Schedule for the order in which materials were presented at the pre-ASTMH workshop)."
  },
  {
    "objectID": "lectures.html#list-of-all-the-lectures-pdfs",
    "href": "lectures.html#list-of-all-the-lectures-pdfs",
    "title": "Lectures",
    "section": "List of all the lectures (PDFs)",
    "text": "List of all the lectures (PDFs)"
  },
  {
    "objectID": "tutorials/module5_sample_size.html",
    "href": "tutorials/module5_sample_size.html",
    "title": "Module 5: Sample size calculation",
    "section": "",
    "text": "Welcome to Module 5: Sample size calculation.\nIn this module, we‚Äôll focus on how to determine the appropriate sample size for your study based on desired power and hypothesis testing parameters. This module will enhance your ability to design studies that are adequately powered to detect meaningful effects.\n\n\nBy the end of this tutorial, you will be able to:\n\nDerive the equation for calculating sample size interactively.\nUnderstand how power relates to sample size calculation.\nInterpret power curves and sample size tables.\nDesign sample sizes based on specific hypotheses."
  },
  {
    "objectID": "tutorials/module5_sample_size.html#introduction",
    "href": "tutorials/module5_sample_size.html#introduction",
    "title": "Module 5: Sample size calculation",
    "section": "",
    "text": "Welcome to Module 5: Sample size calculation.\nIn this module, we‚Äôll focus on how to determine the appropriate sample size for your study based on desired power and hypothesis testing parameters. This module will enhance your ability to design studies that are adequately powered to detect meaningful effects.\n\n\nBy the end of this tutorial, you will be able to:\n\nDerive the equation for calculating sample size interactively.\nUnderstand how power relates to sample size calculation.\nInterpret power curves and sample size tables.\nDesign sample sizes based on specific hypotheses."
  },
  {
    "objectID": "tutorials/module5_sample_size.html#deriving-the-sample-size-formula",
    "href": "tutorials/module5_sample_size.html#deriving-the-sample-size-formula",
    "title": "Module 5: Sample size calculation",
    "section": "Deriving the sample size formula",
    "text": "Deriving the sample size formula\nIn this section, we‚Äôll interactively derive the equation for calculating the sample size required for a study, focusing on the specific hypothesis:\n\nNull hypothesis (H0): p1=p2\nAlternative hypothesis (H1): p1 = p2 + 10%\n\nThis means we‚Äôre interested in detecting a difference of 10 percentage points between two proportions. For example, if we are comparing the prevalence of a drug resistance mutation between two time points, we want to know whether the prevalence remained the same or increased by 10%.\nIMAGE OF SS FORMULA\n\nUnderstanding the components\nBefore we derive the equation, let‚Äôs understand the key components involved in sample size calculation:\n\nEffect size (Œî): The minimum difference between groups that you want to detect.\nSignificance Level (Œ±): The probability of making a Type I error (commonly set at 0.05).\nPower (1 - Œ≤): The probability of correctly rejecting a false null hypothesis (commonly desired at 80% or 0.80).\nStandard deviation (œÉ): A measure of variability in the data.\n\n\n\nDeriving the formula\nWe want to determine the sample size needed to detect a difference in proportions between two groups, where the difference is 10%.\nNow we are going to derive this formula step-by-step to determine this sample size.\nFirst we define our effect size delta:\n\ndelta &lt;- 10"
  },
  {
    "objectID": "tutorials/module5_sample_size.html#bonus",
    "href": "tutorials/module5_sample_size.html#bonus",
    "title": "Module 5: Sample size calculation",
    "section": "Bonus",
    "text": "Bonus\n\nopen from code (do it yourself with hints), repeat for sample size of eg 500"
  },
  {
    "objectID": "tutorials/module5_sample_size.html#resources",
    "href": "tutorials/module5_sample_size.html#resources",
    "title": "Module 5: Sample size calculation",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "tutorials/module3/module3_hypothesis_testing.html",
    "href": "tutorials/module3/module3_hypothesis_testing.html",
    "title": "Module 3: Hypothesis Testing and power",
    "section": "",
    "text": "This activity focuses on hypothesis testing and power. After a short quiz, you will work through two examples; 1) detecting a change in prevalence over time, and 2) looking to detect rare variants. In this activity you will learn:\n\nSome key definitions around null hypothesis testing.\nHow to use a test statistic to decide whether or not to reject a null hypothesis.\nHow to perform power analysis under two different statistical tests, and how to use this to select sample sizes.\nHow to interpret power curves.\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module3/module3_hypothesis_testing.html#learning-outcomes",
    "href": "tutorials/module3/module3_hypothesis_testing.html#learning-outcomes",
    "title": "Module 3: Hypothesis Testing and power",
    "section": "",
    "text": "This activity focuses on hypothesis testing and power. After a short quiz, you will work through two examples; 1) detecting a change in prevalence over time, and 2) looking to detect rare variants. In this activity you will learn:\n\nSome key definitions around null hypothesis testing.\nHow to use a test statistic to decide whether or not to reject a null hypothesis.\nHow to perform power analysis under two different statistical tests, and how to use this to select sample sizes.\nHow to interpret power curves.\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is simulated for teaching purposes only. It does not necessarily represent the real epidemiological situation."
  },
  {
    "objectID": "tutorials/module3/module3_hypothesis_testing.html#short-quiz-on-hypothesis-testing-and-power",
    "href": "tutorials/module3/module3_hypothesis_testing.html#short-quiz-on-hypothesis-testing-and-power",
    "title": "Module 3: Hypothesis Testing and power",
    "section": "Short quiz on hypothesis testing and power",
    "text": "Short quiz on hypothesis testing and power\nQuestion: What is a null hypothesis?\nOptions:\n\nA statement that there is a significant effect or difference between groups.\nA prediction about the future outcome of an experiment.\nA statement that there is no effect or no difference between groups, and any observed effect is due to chance. (correct)\nA hypothesis that describes the expected relationship between two variables.\n\nQuestion: Which of these is not a null hypothesis?\nOptions:\n\nThere is no difference in malaria prevalence between people who sleep under bed-nets and people who do not.\nThe presence of a genetic marker for drug resistance is independent of the region (e.g., East Africa vs.¬†West Africa).\nElevation has no linear relationship with malaria risk.\nMalaria incidence is twice as high in men as it is in women. (correct)\n\nQuestion: A false-negative result is when‚Ä¶(this one takes some thinking)\nOptions:\n\n\nYou fail to reject the null hypothesis when it is actually false. (correct)\n\n\nYou fail to reject the null hypothesis when it is actually true.\n\n\nYou reject the null hypothesis when it is actually false.\n\n\nYou reject the null hypothesis when it is actually true.\n\n\nAnswer: This type of question is always tricky as the language is all quite similar. Remember that a ‚Äúnegative‚Äù result means we fail to reject the null hypothesis. We were looking for something interesting, we didn‚Äôt find it, so we stick with the null. This means we can narrow down the answer to A) or B). Now consider that the question was about a ‚Äúfalse-negative‚Äù, meaning we came to the incorrect conclusion. This means the null hypothesis must have actually been false, and we made the wrong choice by failing to reject it. Therefore, the answer is A).\nQuestion: The parameter \\(\\alpha\\) is often referred to as‚Ä¶\nOptions:\n\nThe confidence level of a hypothesis test.\nThe significance level of a hypothesis test. (correct)\nThe power of a statistical test.\nThe probability of making a Type II error.\n\nQuestion: The value of \\(\\alpha\\) defines‚Ä¶\nOptions:\n\nThe sample size of our study.\nThe chance of getting a false-negative result (also known as a Type II error).\nThe strength of the effect we observe.\nThe chance of getting a false-positive result (also known as a Type I error). (correct)\n\nQuestion: A statistical test that only examines effects in one direction, not both, is called‚Ä¶\nOptions:\n\nA one-headed test.\nA one-way street analysis.\nA wild goose chase.\nA one-tailed test.(correct)\n\nQuestion: TRUE or FALSE, in statistical testing, we always compare our test statistic against the same distribution.\nOptions:\n\nTRUE\nFALSE (correct)\n\nQuestion: You are running a study to test if the prevalence of a drug resistant mutation has changed over time. You will analyse your data using a z-test. The critical values for a two-tailed z-test at the significance level \\(\\alpha = 0.05\\) are at \\(\\pm1.96\\). You obtain a test statistic of -2.54. What should you do based on this result?\nOptions:\n\nReject the null hypothesis because the test statistic is less than zero.\nReject the null hypothesis because the test statistic exceeds the lower critical value. (correct)\nFail to reject the null hypothesis because the test statistic is negative.\nFail to reject the null hypothesis because the test statistic is less than the upper critical value.\n\nAnswer:. We should reject the null hypothesis because we exceed the critical values. In this example, we have two critical values, one negative value at -1.96 and one positive value at +1.96. Anything inside this region (greater than -1/96 but less than +1.96) is reasonably likely under the null hypothesis, but values outside this region are unlikely. Our value was outside this region, meaning we should reject the null hypothesis."
  },
  {
    "objectID": "tutorials/module3/module3_hypothesis_testing.html#testing-for-changes-in-drug-resistance-prevalence-over-time",
    "href": "tutorials/module3/module3_hypothesis_testing.html#testing-for-changes-in-drug-resistance-prevalence-over-time",
    "title": "Module 3: Hypothesis Testing and power",
    "section": "Testing for changes in drug resistance prevalence over time",
    "text": "Testing for changes in drug resistance prevalence over time\nYou are concerned that the prevalence of antimalarial resistance might be increasing in your region. You have data on the prevalence of pfmdr1 N86Y mutations from a cross-sectional survey that took place three years ago. You plan to repeat the survey to establish if there has been a change in the prevalence of N86Y mutations over this time. The statistical test you plan to use is a z-test for proportions.\nQuestion: Which of these is the null hypothesis under this test?\nOptions:\n\nThe prevalence of pfmdr1 N86Y mutations has increased over the past three years.\nThe prevalence of pfmdr1 N86Y mutations has decreased over the past three years.\nThe prevalence of pfmdr1 N86Y mutations is the same now as it was three years ago. correct\nThe prevalence of pfmdr1 N86Y mutations differs between now and three years ago.\n\nThe study three years ago obtained genetic data from 80 samples, of which 12 were identified as carrying the N86Y mutation. We can estimate the prevalence as: \\[\np_1 = \\frac{12}{80} = 0.15\n\\] As part of our power analysis, we have to assume a known value of the prevalence at the present day. We will be pessimistic and assume that the prevalence has doubled over the three years to \\(p_2=0.30\\).\nWe know that the sample size was \\(n_1=80\\) for the first study. For the present study we plan to use a larger sample size of \\(n_2=300\\).\nGiven all of these values, we can calculate the expected value for our test statistic using the following formula: \\[\n\\mu_{\\text{alt}} = \\frac{|p_1 - p_2|}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\n\\] Note that the vertical lines around \\(|p_1 - p2|\\) mean we should take the absolute value of the difference in prevalence. This ensures that \\(\\mu_{\\text{alt}}\\) is always positive.\nComplete the following R code to calculate the value of \\(\\mu_{\\text{alt}}\\):\n\n# input parameters\np1 &lt;- 0.15\np2 &lt;- 0.3\nn1 &lt;- 80\nn2 &lt;- 300\n\n# calculate absolute value of difference in prevalence\np_diff &lt;- abs(p1 - p2)\n\n# calculate the standard error\nSE &lt;- sqrt(p1*(1 - p1) / n1 + p2*(1 - p2) / n2)\n\n# calculate mu_alt\nmu_alt &lt;- p_diff / SE\n\nprint(mu_alt)\n\n[1] 3.131975\n\n\nWe can use the value of \\(\\mu_{\\text{alt}}\\) to tell us our power. The formula for power under the z-test is:\n\\[\nP_{ow} = 1 - \\phi(z_{1-\\alpha/2} - \\mu_{\\text{alt}})\n\\]\nIn this formula, \\(\\phi(x)\\) refers to the area under the curve of a standard normal distribution from \\(-\\infty\\) up to the point \\(x\\). There is no simple way of calculating this value, but we can obtain it easily in R using the pnorm() function. As in previous activities, the value \\(z_{1 - \\alpha/2}\\) refers to the critical value of the normal distribution at a significance level \\(\\alpha\\) (two-tailed), which is approximately equal to 1.96.\nComplete the following R code to calculate the power under the planned study design:\n\n# define significance level and calculate z\nalpha &lt;- 0.05\nz &lt;- qnorm(1 - alpha / 2)\n\n# calculate power\npower &lt;- 1 - pnorm(z - mu_alt)\nprint(power)\n\n[1] 0.8794036\n\n\nFrom this calculation, we can see that our power is around 88%.\nQuestion: A power of 88% means‚Ä¶\nOptions:\n\nThere is an 88% chance that the study will be successful.\nThere is an 88% difference in prevalence between the time points.\nThere is an 88% chance that the alternative hypothesis is true.\nAssuming the alternative hypothesis is true, there is an 88% chance that we will correctly reject the null hypothesis. correct\n\nWe normally aim for 80% power, meaning this study design is adequately powered. In fact, we could argue that this study is slightly over-powered, meaning we could get away with using fewer samples and still be good enough.\nThe following power curve shows us the power as a function of the sample size (\\(n_2\\)). The region with 80% power or above is shaded in blue.\n\n\n\n\n\nQuestion: From this graph, what sample size is needed to achieve a power of 80%?\nOptions:\n\n140\n165 correct\n190\n285\n\nOnly 165 samples are needed for us to obtain 80% power. From the power curve, we can see that we reach diminishing returns as we continue increasing the sample size, meaning there is little benefit to continually driving up the sample size. The conclusion of our power analysis is that our initial plan to sequence 300 samples was excessive, and we can get away with sequencing far fewer samples. This is likely to lead to cost savings, and may allow us to redirect these resources elsewhere."
  },
  {
    "objectID": "tutorials/module3/module3_hypothesis_testing.html#detecting-rare-variants",
    "href": "tutorials/module3/module3_hypothesis_testing.html#detecting-rare-variants",
    "title": "Module 3: Hypothesis Testing and power",
    "section": "Detecting rare variants",
    "text": "Detecting rare variants\nYou are planning a study to look for the presence of validated pfk13 mutations. You are not interested in estimating the prevalence of mutations, rather you want to know if any of these mutations are present in your population. You will test people for malaria as they present to a local health facility, and a subset of dried blood spots from malaria-positive patients will be sent away for sequencing. You only have the resources to sequence 100 samples, and you want to know if it is worth conducting a study with such a small sample size.\nThis type of presence-absence study can be framed as a hypothesis test. The null hypothesis is that there are no validated mutations present in the population, i.e.¬†the prevalence of these mutations is zero. Even a single observation of a mutation would disprove this null hypothesis, therefore - unlike most statistical tests (e.g.¬†the z-test above) - there is no test statistic to calculate here. Instead, we simply reject the null hypothesis if we see a single validated mutant, otherwise we fail to reject the null hypothesis.\nWe start by assuming a known prevalence, \\(p\\), for the validated mutations. We also need to define our sample size, \\(n\\). Samples are assumed to be drawn at random from the population.\nThe probability that a single sample is negative for mutants is \\(1-p\\). The probability that all \\(n\\) samples are negative is \\((1 - p)^n\\). The probability of seeing at least one mutant is one minus the probability of seeing no mutants. The probability of seeing at least one mutant is also defined as our power under this test, therefore:\n\\[\nP_{ow} = 1 - (1 - p)^n\n\\]\nComplete the following R code to calculate the power under an assumed prevalence of \\(p=0.05\\) and a sample size of \\(n=100\\):\n\n# define parameters\np &lt;- 0.05\nn &lt;- 100\n\n# calculate power\npower &lt;- 1 - (1 - p)^n\nprint(power)\n\n[1] 0.9940795\n\n\nWe have extremely high power under this example, which is reassuring.\nQuestion: What is the power if we assume the prevalence of validated mutants is only 1% in the population?\nOptions:\n\n51%\n63% correct\n76%\n84%\n\nThe following power curves show power as a function of sample size for a range of different value of \\(p\\):\n\n\n\n\n\nQuestion: From these power curves, what sample size is needed to achieve 80% power if we assume the true prevalence of mutants in the population is 2%?\nOptions:\n\n40\n60\n80 correct\n90\n\nQuestion: What about if we assume the prevalence is 1%?\nOptions:\n\n16\n32\n80\nMore than 100, meaning we cannot achieve 80% power within our resource limits.correct\n\nOur original question was whether it was worth conducting this study given our tight resource constraints of at most 100 samples sequenced. What the graph shows us is that whether or not it is worth running this study depends on what we are happy to assume about the prevalence of mutations in the population. If we are very worried about missing mutations present at only 1% in the population, then it is not worth running this study (unless we have some other indirect benefits of the sequencing) as we will be under-powered. On the other hand, if we would not be very concerned about mutations at 1% prevalence, and we are much more concerned about them reaching 5% prevalence or higher, then we would achieve adequate power with as few as 32 samples. This is the point at which statistical information needs to be combined with clinical and programmatic information to make an informed decision about the most appropriate course of action."
  },
  {
    "objectID": "tutorials/module3/module3_hypothesis_testing.html#bonus-questions",
    "href": "tutorials/module3/module3_hypothesis_testing.html#bonus-questions",
    "title": "Module 3: Hypothesis Testing and power",
    "section": "Bonus questions",
    "text": "Bonus questions\nA study was conducted with the aim of detecting rare pfk13 variants. A sample size of 32 was used, as this was determined to be sufficient based on statistical arguments. However, during sequencing it also became clear that pfmdr1 mutations could be sequenced from the same samples at little extra cost. 10 out of the 32 samples were found to carry the pfmdr1 N86Y mutation, giving a prevalence of 32%. This was surprising - only last year a study had found a prevalence of 20% in a sample of 150 people. This raised the question of whether the prevalence of N86Y mutations had increased significantly over this period.\nThe research team carried out a z-test for proportions to see if there was a statistically significant increase between the two studies. Here are the inputs to their test:\n\\[\n\\begin{align}\n\\hat{p}_1 &= \\frac{30}{150} = 0.20 \\\\\n\\hat{p}_2 &= \\frac{10}{32} = 0.31 \\\\\n\\hat{p} &= \\frac{30 + 10}{150 + 32} = 0.22 \\\\\nn_1 &= 150 \\\\\nn_2 &= 32\n\\end{align}\n\\] The test statistic was calculated from these values as follows: \\[\nZ = \\frac{|\\hat{p}_1 - \\hat{p}_2|}{\\sqrt{\\hat{p}(1 - \\hat{p})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\]\nThis gives the value \\(Z = 1.40\\). The researchers compared this value against the critical values of \\(\\pm1.96\\) and found that this was non-significant. Therefore, they did not have evidence to reject the null hypothesis that prevalence stayed the same over this time period. They took this as reassurance that prevalence of N86Y mutations is not increasing in the region.\nHowever‚Ä¶\nQuestion: Were they right to come to this conclusion? What was their power to detect a a change in prevalence from 20% to 30%? What about a change from 20% to 40%? How does this ‚Äúretrospective‚Äù power analysis change how we interpret the non-significant finding from the z-test?\nAnswer:\nWhen we conduct power analysis under the z-test, we find that power to detect a change from 20% to 30% was only around 21%. Similarly, power to detect a change from 20% to 40% was still only 58%. This falls well below the usual 80% power threshold. With this low power there would be a very good chance of getting a non-significant result even if prevalence had doubled!\nThe problem here is that the sample size of 32 is too small, leading to an underpowered study. This happened because the study design was not powered to answer questions about the change in prevalence of N86Y mutations, rather it was designed and powered to answer a question about presence/absence of rare pfk13 variants.\nThe lack of power means we shouldn‚Äôt read too much into the non-significant finding. There could have been major changes in the prevalence of N86Y mutations over the year, but we would have very little chance of rejecting the null hypothesis due to our lack of power.\nQuestion: How could this low power have been avoided?\nAnswer: If our plan was to look for changes in prevalence of N86Y mutations alongside looking for rare pfk13 mutants then this should have been accounted for at the design stage. A power analysis should have been conducted that took into account both endpoints. This would have resulted in a larger sample size that was sufficient to answer both questions."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html",
    "href": "tutorials/module2/module2_interactive.html",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "",
    "text": "Welcome to Module 2: Calculating Sample Size from Margin of Error\nIn this module, we‚Äôll explore how margin of error (MOE) arguments can guide sample size determination. This approach differs from power analysis, as it doesn‚Äôt involve hypothesis testing and therefore doesn‚Äôt require the concept of power. Instead, it relies solely on the concept of precision. Larger sample sizes lead to more precise estimates. This method is invaluable not only in Malaria Molecular Surveillance (MMS) but also in epidemiology more broadly, where estimating prevalence is often a primary goal.\n\n\nBy the end of this tutorial, you will be able to:\n\nCalculate a 95% confidence interval from prevalence data.\nCalculate a minimum sample size using assumptions about prevalence and margin of error.\nAccount for drop-out and malaria positive fraction.\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is artificial and designed for teaching purposes only. It does not necessarily represent the real epidemiological situation in these places."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html#introduction",
    "href": "tutorials/module2/module2_interactive.html#introduction",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "",
    "text": "Welcome to Module 2: Calculating Sample Size from Margin of Error\nIn this module, we‚Äôll explore how margin of error (MOE) arguments can guide sample size determination. This approach differs from power analysis, as it doesn‚Äôt involve hypothesis testing and therefore doesn‚Äôt require the concept of power. Instead, it relies solely on the concept of precision. Larger sample sizes lead to more precise estimates. This method is invaluable not only in Malaria Molecular Surveillance (MMS) but also in epidemiology more broadly, where estimating prevalence is often a primary goal.\n\n\nBy the end of this tutorial, you will be able to:\n\nCalculate a 95% confidence interval from prevalence data.\nCalculate a minimum sample size using assumptions about prevalence and margin of error.\nAccount for drop-out and malaria positive fraction.\n\nDisclaimer: The scenarios in this document are entirely fictitious. While real place names are used, the data itself is artificial and designed for teaching purposes only. It does not necessarily represent the real epidemiological situation in these places."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html#background",
    "href": "tutorials/module2/module2_interactive.html#background",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Background",
    "text": "Background\nYou have been recruited by the National Malaria Control Programme (NMCP) of the Democratic Republic of the Congo (DRC) to assist with study design. The NMCP is concerned about the potential spread of mutations that confer partial resistance to the drug combination Sulfadoxine-Pyrimethamine (SP). The dhps K540E mutation, known to be associated with high level SP resistance when found alongside other common mutations, has recently been found at high prevalence (72%) in neighbouring Uganda. In the last few weeks there have been anecdotal reports of SP failure in Rutshuru town, which lies in Eastern DRC close to the border with Uganda. The NMCP is concerned about possible flow of drug resistant parasites over the border.\nThe NMCP plans to conduct a cross-sectional study to estimate the prevalence of the dhps K540E mutation within Rutshuru town. Your job is to work out the appropriate sample size for this study."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html#results-of-a-pilot-study",
    "href": "tutorials/module2/module2_interactive.html#results-of-a-pilot-study",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Results of a pilot study",
    "text": "Results of a pilot study\nA pilot study has already been conducted in Rutshuru. This pilot study included 100 participants chosen at random from households within the town, who were tested for malaria via rapid diagnostic test (RDT). 23 people tested positive for malaria and these samples were sent away for genetic sequencing. 19 samples were successfully sequenced, of which 5 were positive for the K540E mutation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall that we can use the following formula to calculate a 95% confidence interval on our prevalence estimate:\n\\[\n\\hat{p} \\pm z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}\n\\] Complete the following R code to compute this interval:\n\n\n# estimate the prevalence\nx &lt;- \nN &lt;- \np &lt;- x / N\n\n# calculate the margin of error (MOE)\nz &lt;- 1.96\nMOE &lt;- z*sqrt(p*(1 - p) / N)\n\n# compute lower and upper 95% limits\np - MOE\np + MOE\n\n\n\n\n\n# estimate the prevalence\nx &lt;- 5\nN &lt;- 19\np &lt;- x / N\n\n# calculate the margin of error (MOE)\nz &lt;- 1.96\nMOE &lt;- z*sqrt(p*(1 - p) / N)\n\n# compute lower and upper 95% limits\nCI_lower &lt;- p - MOE\nCI_upper &lt;- p + MOE\nc(CI_lower, CI_upper)\n\n\n\n\nClick to see the answer\n\nThe 95% interval goes from 0.065 to 0.461, or in other words from 6.5% to 46.1%.\n\n\nThe 95% confidence interval reveals considerable uncertainty regarding the prevalence of K540E mutations. While our best estimate is 26.3%, the plausible range spans from 6.5% to 46.1%, which the NMCP finds too broad to be practically useful. They plan to conduct a follow-up study to obtain a more precise estimate."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html#calculating-the-appropriate-sample-size",
    "href": "tutorials/module2/module2_interactive.html#calculating-the-appropriate-sample-size",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Calculating the appropriate sample size",
    "text": "Calculating the appropriate sample size\nWhen designing the new study we will calculate the exact sample size needed to achieve a target margin of error (MOE). We can do this by rearranging the MOE formula to isolate the sample size (N) on the left side. If you‚Äôre comfortable with the math and would like to see the steps, follow along below. Otherwise, feel free to skip to Step 3 for the final formula.\nStep 1: Write down the formula for the MOE\nWe will use the mathematical symbol \\(m\\) for the MOE:\n\\[\nm = z_{1-\\alpha/2}\\sqrt{\\frac{p(1-p)}{N}}\n\\] Step 2: Square both sides\n\\[\nm^2 = z_{1-\\alpha/2}^2 \\frac{p(1-p)}{N}\n\\] Step 3: Multiply both sides by \\(N\\) and divide by \\(m^2\\)\n\\[\nN = z_{1-\\alpha/2}^2 \\frac{p(1-p)}{m^2}\n\\]\nNow we have a new formula that we can use to determine the appropriate sample size based on assumed values of \\(p\\) and \\(m\\). Our reason for working through the derivation of this formula is to show how closely connected it is to the formula for the confidence interval. In fact, it is the same mathematical expression, just ‚Äúreverse engineered‚Äù to be in terms of \\(N\\).\nNow we need to decide what values to assume for \\(p\\) and \\(m\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the sake of this tutorial we will assume a value of \\(p=0.26\\) to match the pilot data. The NMCP has decided that a MOE of 5% is acceptable. Complete the following R code to compute the resulting sample size:\n\n\n# enter assumed values\np &lt;- \nm &lt;- \n\n# calculate the raw sample size\nz &lt;- 1.96\nz^2*p*(1 - p) / m^2\n\n\n\n\n\n# enter assumed values\np &lt;- 0.26\nm &lt;- 0.05\n\n# calculate the raw sample size\nz &lt;- 1.96\nz^2*p*(1 - p) / m^2\n\n\n\n\nClick to see the answer\n\nWe obtain a value of 295.65. We would round this up to \\(N=296\\) to give a whole number.\n\n\nOne of the nice things about sample size determination is that we can easily check that our calculation is correct. Optional exercise: Try entering the values \\(p=0.26\\) and \\(N=296\\) into the 95% CI formula that we used in the pilot data analysis. If our calculations were correct, you should find that the resulting MOE is very close to 5%.\n\n\n# Copy over the Wald formula code from the previous section, and edit to reflect\n# the new assumed prevalence of 26% and sample size of 296\n\n\n\n\n\n# enter assumed values\np &lt;- 0.26\nN &lt;- 296\n\n# calculate the margin of error (MOE)\nz &lt;- 1.96\nz*sqrt(p*(1 - p) / N)\n\n\n\n\nClick to see the answer\n\nYou should obtain a margin of error of 4.997%, which is very close to the target 5%. Notice that our MOE will always be equal or smaller than the target MOE. This is because we rounded the sample size up from 295.65 to 296."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html#buffering",
    "href": "tutorials/module2/module2_interactive.html#buffering",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Buffering",
    "text": "Buffering\nBuffering refers to increasing a sample size to allow for drop-out (loss of samples). Some ways that drop-out can occur are through:\n\nParticipants withdrawing consent\nParticipants dying or leaving the area\nSamples being lost during transportation\nSamples becoming contaminated\nSamples failing amplification or sequencing resulting in a lack of genetic data\nData being lost due to data storage errors\n\nWe cannot completely eliminate the risk of drop-out, but by buffering sample sizes we can at least be robust to it. If we expect a proportion \\(d\\) of samples to be lost, then the formula for buffered sample size is:\n\\[\nN_{\\text{buffered}} = \\frac{N_{\\text{original}}}{1 - d}\n\\]\nThrough consulting with lab technicians and the study team, you estimate that 10% of samples may be lost to drop-out. Complete the following R code to come up with a buffered sample size:\n\n\n# enter sample size and estimated drop-out\nN &lt;- 296\nd &lt;- 0.1\n\n# calculate the buffered sample size\nN_buffered &lt;- \n\nprint(N_buffered)\n\n\n\n\n\n# enter sample size and estimated drop-out\nN &lt;- 296\nd &lt;- 0.1\n\n# calculate the buffered sample size\nN_buffered &lt;- N / (1 - d)\n\nprint(N_buffered)\n\n\n\n\nClick to see the answer\n\nYou should obtain a buffered sample size of 328.89, which we would round up to 329."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html#accounting-for-positive-fraction",
    "href": "tutorials/module2/module2_interactive.html#accounting-for-positive-fraction",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Accounting for positive fraction",
    "text": "Accounting for positive fraction\nSo far, we have focused on working out how many confirmed malaria cases we need in our study. However, recall that this will be a cross-sectional study with individuals being sampled at random from households within Rutshuru town. Many of the individuals tested will be negative for malaria. It may be useful for the study team to know how many individuals they need to test as part of this study, which may be considerably higher than the number of confirmed malaria cases.\nThe NMCP estimates that 25% of the population of Rutshuru will be positive for malaria by RDT. We can use the same buffering formula as before, but now using the positive fraction (\\(f\\)) to inflate our sample size:\n\\[\nN_{\\text{test}} = \\frac{N_{\\text{confirmed}}}{f}\n\\]\nNote that this is in addition to buffering for drop-out. We can imagine a chain of events where we can lose samples at each stage; we want to ensure that in the end we still have enough samples remaining.\nComplete the following R code to work out the number of people we will need to test to achieve the final target sample size:\n\n\n# enter buffered sample size and positive fraction\nN &lt;- \nf &lt;- \n\n# calculate the testing sample size\nN_test &lt;- \n\nprint(N_test)\n\n\n\n\n\n# enter buffered sample size and positive fraction\nN &lt;- 329\nf &lt;- 0.25\n\n# calculate the testing sample size\nN_test &lt;- N / f\n\nprint(N_test)\n\n\n\n\nClick to see the answer\n\nYou should find that 1316 people need to be tested.\n\n\nYou have now completed your study design exercise. Your recommendation to the NMCP is as follows:\nAssuming a prevalence of K540E mutations of 26% based on pilot data, a sample size of 329 confirmed malaria cases will be needed to estimate prevalence to within 5% margin of error. This number is buffered to allow for 10% drop-out.\nAssuming that malaria prevalence is 25% by RDT in Rutshuru town, this translates to 1316 individuals who will need to be tested in the cross-sectional study design."
  },
  {
    "objectID": "tutorials/module2/module2_interactive.html#bonus-questions",
    "href": "tutorials/module2/module2_interactive.html#bonus-questions",
    "title": "Module 2: Calculating Sample Size from Margin of Error",
    "section": "Bonus questions",
    "text": "Bonus questions\nThe study design above is based on strong statistical principles. However, it is worth testing how robust these numbers are to changes in our assumptions.\nUnder the chosen sample size of 296 (after drop-out), what would be your margin of error under the worst case scenario that the true prevalence of the K540E mutation was actually 50%?\n\n\n\n\n\n\n\n# enter assumed values\np &lt;- 0.5\nN &lt;- 296\n\n# calculate the margin of error (MOE)\nz &lt;- 1.96\nz*sqrt(p*(1 - p) / N)\n\n\n\n\nClick to see the answer\n\nThe MOE would increase to 5.7% in the most pessimistic scenario.\n\n\nWe estimated that 1316 people will need to be tested based on an assumed 25% prevalence of malaria. But what if malaria prevalence is actually only 15% in Rutshuru town? What would be your expected final sample size, and what would be your resulting MOE?\n\n\n\n\n\n\n\n# enter assumed values\np &lt;- 0.26\nN_test &lt;- 1316\n\n# work out final sample size assuming 15% prevalence and 10% drop-out\nN &lt;- round(N_test * 0.15 * 0.9)\nprint(N)\n\n# calculate the margin of error (MOE)\nz &lt;- 1.96\nz*sqrt(p*(1 - p) / N)\n\n\n\n\nClick to see the answer\n\nAssuming 15% prevalence and 10% drop-out, our final sample size of successfully sequenced malaria cases would be around 178. This would result in a MOE of 6.4%."
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html",
    "href": "tutorials/module1/module1_sampling_interactive.html",
    "title": "Module 1: Sampling from a population",
    "section": "",
    "text": "Welcome to Module 1: Sampling from a population.\nIn this module, we‚Äôll explore how to relate samples to populations, calculate confidence intervals, and understand the impact of sample size on study results. We are going to explore how to think about this when conducting studies to determine the prevalence of malaria to start with a simple example to gently introduce these statistical concepts. When we think about malaria molecular surveillance (MMS) studies, the same concepts apply. For example, in MMS studies we might be designing our study to determine the prevalence of molecular markers (for example, drug resistance markers) rather than malaria infection.\n\n\nBy the end of this tutorial, you will be able to:\n\nDefine the target population for a study.\nDifferentiate between a population and a sample.\nCalculate the 95% confidence interval.\nAssess how sampling variability impacts the representation of the population.\nUnderstand the effect of sample size on confidence intervals."
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html#introduction",
    "href": "tutorials/module1/module1_sampling_interactive.html#introduction",
    "title": "Module 1: Sampling from a population",
    "section": "",
    "text": "Welcome to Module 1: Sampling from a population.\nIn this module, we‚Äôll explore how to relate samples to populations, calculate confidence intervals, and understand the impact of sample size on study results. We are going to explore how to think about this when conducting studies to determine the prevalence of malaria to start with a simple example to gently introduce these statistical concepts. When we think about malaria molecular surveillance (MMS) studies, the same concepts apply. For example, in MMS studies we might be designing our study to determine the prevalence of molecular markers (for example, drug resistance markers) rather than malaria infection.\n\n\nBy the end of this tutorial, you will be able to:\n\nDefine the target population for a study.\nDifferentiate between a population and a sample.\nCalculate the 95% confidence interval.\nAssess how sampling variability impacts the representation of the population.\nUnderstand the effect of sample size on confidence intervals."
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html#what-is-my-population",
    "href": "tutorials/module1/module1_sampling_interactive.html#what-is-my-population",
    "title": "Module 1: Sampling from a population",
    "section": "What is my population?",
    "text": "What is my population?\nIn any study, clearly defining the target population is crucial. Let‚Äôs consider a few examples of made-up studies.\n\n\nQUIZ - Target population"
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html#relating-the-sample-to-the-population",
    "href": "tutorials/module1/module1_sampling_interactive.html#relating-the-sample-to-the-population",
    "title": "Module 1: Sampling from a population",
    "section": "Relating the sample to the population",
    "text": "Relating the sample to the population\nIt is often not feasible to sample the entire population, for example, due to costs or difficulty in sampling everyone. The good thing is we can design our study so that our target population is a representative sample of our population.\nOften when designing epidemiological studies, different data sources can provide valuable information on our population, for example a Demographic and Health Survey (DHS) or a population census.\n\nUsing the population census\nLet‚Äôs go back to our made-up study.\nWe now have access to a census of the entire population of the village (N= 10000) with information on each residents age and sex. For purposes of this tutorial we ‚Äúknow‚Äù the true infection status of every individual, there are 2678 people in the village with malaria infections. This will help us understand how our sample relates to the entire population.\nThis is the information we have in the census (here we show the first 6 residents in the census):\n\n\n\n\n\nid\nage\nsex\nmalaria_infection\n\n\n\n\n1\n27\nMale\nNot infected\n\n\n2\n79\nFemale\nNot infected\n\n\n3\n21\nFemale\nInfected\n\n\n4\n8\nFemale\nNot infected\n\n\n5\n4\nMale\nNot infected\n\n\n6\n37\nMale\nInfected\n\n\n\n\n\n\n\nPopulation demographics\nBelow we can see a breakdown of malaria-infected individuals in this population.\n\n\n\n\n\nLet‚Äôs look at the age and sex distribution.\n\n\n\n\n\n\n\nSampling from the population\nSuppose we have resources to sequence 200 samples. Let‚Äôs randomly sample 200 individuals from the population and see how they ‚Äòcompare‚Äô.\nLet‚Äôs take a look at the demographics of our sample by plotting the age and sex distribution of the population and the sample. On the left we have the same age/sex distribution we saw above for the whole population, and on the right we see the distribution in our sampled individuals.\nBelow we use the function sampleFromPopulation() and we specify what our sample size is and from where we want to sample (in our case from the census data).\n\n  Reflection: \n Do we see a similar age and sex distribution? What about the proportion of infected and not infected individuals in the population? \n\n We can use the plotAgeSexDistribution() and plotInfectedProportion() functions to visualize our results. Click on ‚ÄúRun Code‚Äù.\n\n\nsample &lt;- sampleFromPopulation(sample_size = 200, census)\ncomparison &lt;- compareSampleToPopulation(sample, census)\n\nplotAgeSexDistribution(comparison)\n\nplotInfectedProportion(comparison)\n\n\n\n\n\nSampling many times from the population\nNow run this a few times with the sample size of 200 to see how it changes with every random sample. Click ‚ÄúStart Over‚Äù and then ‚ÄúRun code‚Äù.\n\n\nsample &lt;- sampleFromPopulation(sample_size = 200, census)\ncomparison &lt;- compareSampleToPopulation(sample, census)\n\nplotAgeSexDistribution(comparison)\nplotInfectedProportion(comparison)\n\n\n\n\n\nQUIZ - Sampling from the population\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Reflection: \n Smaller sample sizes are more susceptible to sampling variability. With a limited number of individuals, the likelihood of the sample deviating from the population characteristics increases. Think about how this may or may not impact your results. \n\n\n\n‚ú® BONUS QUESTION ‚ú®\nYou will have noticed from our exploration above that the sample differs from the population and it doesn‚Äôt always have the same age and sex distribution.\n\n\nQUIZ - Sampling bias"
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html#estimating-prevalence-in-our-sample-and-calculating-the-95-confidence-interval",
    "href": "tutorials/module1/module1_sampling_interactive.html#estimating-prevalence-in-our-sample-and-calculating-the-95-confidence-interval",
    "title": "Module 1: Sampling from a population",
    "section": "Estimating prevalence in our sample and calculating the 95% confidence interval",
    "text": "Estimating prevalence in our sample and calculating the 95% confidence interval\nOur next topic focuses on calculating the 95% confidence interval (CI) using the Wald method. When we estimate the prevalence of malaria in our sample we obtain a point estimate. We also need to calculate its 95% CI to understand the variation around our estimate. The CI provides an interval with lower and upper bounds and in our case, it means that we are 95% confident that the true population prevalence lies within this interval. If we were to repeat the sampling process many times and calculate a confidence interval each time, approximately 95 out of 100 of these intervals would contain the true population prevalence.\nBelow we will learn about the Wald CI formula and how we can calculate it in practice.\n\nEstimating prevalence in our sample\nFor this exercise we have already pre-calculated some useful parameters:\n\nDefined sample_size to be 200\nUsed the function sampleFromPopulation() to select 200 individuals at random from our census\nWe counted the number of infected individuals in our sample and defined it as infected_count (in our example it is 69 individuals)\n\nBelow is the code we ran for reference, but you don‚Äôt have to run it yourself as everything is already loaded.\n\n# set the sample size\nsample_size &lt;- 200\n\n# sample from the population\nsample_data &lt;- sampleFromPopulation(sample_size, census) \n\n# Count number of infected individuals in the sample\ninfected_count &lt;- sum(sample_data$malaria_infection == \"Infected\")\n\n\n\nWhat is the estimated prevalence of malaria in our sample?\nWe can calculate this by dividing the number of individuals infected with malaria by our sample size.\nTry coding it yourself or click on the solution. Note: In R when we want to divide two things we can use /.\n\n\n\n\n\n\n\n69 / 200\n\n# Or you can use the stored variables:\ninfected_count / sample_size\n\n\n\n\nClick to see the answer\n\nOur estimated prevalence is 0.345 or 34.5%.\n\n\nCalculating the 95% CI\nAs we saw above, we need to also calculate the 95% CI around our estimate.\nThis is the Wald confidence interval formula:\n\nWe are now going to go through this formula step-by-step!\n\n1. Defining our sample proportion, p_hat\nThe sample proportion refers to the proportion of infected individuals in our sample. We just calculated this above by dividing the number of infected individuals in the sample by the total sample size. Let‚Äôs do it again for good measure, and record it as p_hat.\n\n\np_hat &lt;-\n\n\n\n\n\n# Sample proportion\np_hat &lt;- infected_count / sample_size\np_hat\n\n\n\n2. Sample size, n\nAbove we defined our sample size to be n=200 and recorded it as sample_size.\n\n\n3. Calculating the standard error\nNow we can calculate the standard error using p_hat and sample_size. In our formula, the standard error is the part that is multiplied by Z (see below).\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nNote: In R the function to take the square root is sqrt() and when we want to multiply two things we can use *.\n\n\nSE &lt;- \n\n\n\n\n\n# Standard error\nSE &lt;- sqrt((p_hat * (1 - p_hat)) / sample_size)\nSE\n\n\n\n\nClick to see the answer\n\nOur standard error is 0.336.\n\n\n4. Calculating the confidence interval\nNow that we know our standard error, we can calculate the lower and upper bounds of our 95% CI. We use the Z-score for 95% confidence, which is 1.96 and record it as Z. Then we need to calculate our lower and upper bounds using the formula above. We need to multiply Z by our SE and substract it from p_hat.\nLet‚Äôs start with the lower bound. Click on the solution if you need help.\n\n\n# Z-score for 95% confidence\nZ &lt;- 1.96\n\n# Lower bound\n\n\n\n\n\n# Z-score for 95% confidence\nZ &lt;- 1.96\n\n# Lower bound\np_hat - Z * SE\n\n\n\n\nClick to see the answer\n\nOur lower bound is 0.279 or 27.9%.\n\nNow let‚Äôs calculate the upper bound. Remember now we need to add instead of substract.\n\n\n# Z-score for 95% confidence\nZ &lt;- 1.96\n\n# Upper bound\n\n\n\n\n\n# Z-score for 95% confidence\nZ &lt;- 1.96\n\n# Upper bound\np_hat + Z * SE\n\n\n\n\nClick to see the answer\n\nOur upper bound is 0.411 or 41.1%.\n\nLet‚Äôs put it all together!\n\n\ninfected_count &lt;- 69\nsample_size &lt;- 200\nZ &lt;- 1.96\n\np_hat &lt;- infected_count / sample_size\nSE &lt;- sqrt((p_hat * (1 - p_hat)) / sample_size)\nCI_lower &lt;- p_hat - Z * SE\nCI_upper &lt;- p_hat + Z * SE\n\n# Print our values\np_hat\nCI_lower\nCI_upper\n\n\n\nSo, putting it all together, our estimated prevalence is 0.345 or 34.5% and our 95%CI is 27.9% to 41.1%."
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html#comparing-to-the-true-prevalence-estimate",
    "href": "tutorials/module1/module1_sampling_interactive.html#comparing-to-the-true-prevalence-estimate",
    "title": "Module 1: Sampling from a population",
    "section": "Comparing to the true prevalence estimate",
    "text": "Comparing to the true prevalence estimate\nYou may remember from our exploration of the census data earlier, that 26.78% of our population was infected with malaria, in others this is the true prevalence.\n\n\nQUIZ - 95% confidence interval\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Reflection: \n Do you always expect the true prevalence to fall within the 95%CI?"
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html#how-often-does-true-prevalence-fall-within-the-95ci",
    "href": "tutorials/module1/module1_sampling_interactive.html#how-often-does-true-prevalence-fall-within-the-95ci",
    "title": "Module 1: Sampling from a population",
    "section": "How often does true prevalence fall within the 95%CI?",
    "text": "How often does true prevalence fall within the 95%CI?\nWe explored above what would happen if we randomly sampled 200 individuals. In this first example, the true prevalence didn‚Äôt fall within the 95%CI. But this was just one example. Now we want to see what happens if we repeat this sampling many times. Let‚Äôs now explore by running a simulation where we sample 1000 times and we will count how many times our true prevalence is within the 95%CI.\n\n  Reflection: \n Before you run the below code, think about the intuition behind this - how often do you expect the true prevalence to be within the 95%CI? \n\nNow run the code and see if you were correct!\n\n\nn_simulations &lt;- 1000\nsample_size &lt;- 200\n\nresults &lt;- replicate(n_simulations, simulate_CI(census, sample_size, true_prevalence))\n\nplotCISimulationResults(results, n_simulations)\n\n\n\n\n\nQUIZ - Confidence Intervals and Simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter running the simulation, you should notice that the true prevalence falls within the 95% confidence intervals in approximately 95% of the simulations. This outcome aligns with the definition of a 95% confidence interval: if we were to repeat the sampling process many times, we would expect the true parameter to lie within the calculated confidence interval about 95 out of 100 times.\n\n  Reflection: \n This simulation demonstrates the concept of confidence level in statistical inference. It shows that the method we use to calculate confidence intervals is reliable in the long run. However, in any single sample (like the one we initially took), there‚Äôs still a chance (about 5%) that the true prevalence will not be captured within the interval. This is why it‚Äôs important to interpret confidence intervals correctly and understand that they provide a measure of the uncertainty associated with our estimates."
  },
  {
    "objectID": "tutorials/module1/module1_sampling_interactive.html#bonus",
    "href": "tutorials/module1/module1_sampling_interactive.html#bonus",
    "title": "Module 1: Sampling from a population",
    "section": "Bonus",
    "text": "Bonus\nNote: This section is optional and requires more coding than the previous exercise\nLet‚Äôs repeat this exercise for a sample size of 500. Try coding it yourself from scratch using the functions that we used above. Click on the solution if you get stuck!\n\n  Reflection: \n Does the true prevalence (26.78%) fall within our 95% CI? What do you notice about the simulation results? \n\n\n\nsample_size &lt;- 500\n\n\n\n\n\nsample_size &lt;- 500\nsample_data &lt;- sampleFromPopulation(sample_size, census)\ncomparison &lt;- compareSampleToPopulation(sample_data, census)\n\nplotAgeSexDistribution(comparison)\nplotInfectedProportion(comparison)\n\ninfected_count &lt;- sum(sample_data$malaria_infection == \"Infected\")\n\np_hat &lt;- infected_count / sample_size\nSE &lt;- sqrt((p_hat * (1 - p_hat)) / sample_size)\nCI_lower &lt;- p_hat - Z * SE\nCI_upper &lt;- p_hat + Z * SE\n\ncheckPrevalenceCI(true_prevalence, CI_lower, CI_upper)\n\nn_simulations &lt;- 1000\nresults &lt;- replicate(n_simulations, simulate_CI(census, sample_size, true_prevalence))\nplotCISimulationResults(results, n_simulations)"
  }
]